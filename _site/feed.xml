<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-29T08:31:08+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hudanyun Sheng / Data Scientist</title><subtitle>personal description</subtitle><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><entry><title type="html">Financial Credit Risk Management</title><link href="http://localhost:4000/posts/2024/04/risk-management" rel="alternate" type="text/html" title="Financial Credit Risk Management" /><published>2024-04-02T00:00:00+08:00</published><updated>2024-04-02T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/04/financial-credit-risk-management</id><content type="html" xml:base="http://localhost:4000/posts/2024/04/risk-management"><![CDATA[<h1 id="ä¿¡è´·ä¸šåŠ¡æ¨¡å‹ç»´åº¦">ä¿¡è´·ä¸šåŠ¡æ¨¡å‹ç»´åº¦</h1>

<ol>
  <li>
    <h2 id="è´·å‰æ¨¡å‹">è´·å‰æ¨¡å‹</h2>
    <p>Aå¡ (Application scorecard) ç”³è¯·è¯„åˆ†å¡</p>
  </li>
  <li>
    <h2 id="è´·ä¸­æ¨¡å‹">è´·ä¸­æ¨¡å‹</h2>
    <p>Bå¡ (Behavior scorecard) è¡Œä¸ºè¯„åˆ†å¡</p>
  </li>
  <li>
    <h2 id="è´·åæ¨¡å‹">è´·åæ¨¡å‹</h2>
    <p>Cå¡ (Collection scorecard) å‚¬æ”¶è¯„åˆ†å¡</p>
  </li>
  <li>
    <h2 id="åæ¬ºè¯ˆæ¨¡å‹">åæ¬ºè¯ˆæ¨¡å‹</h2>
  </li>
  <li>
    <h2 id="ç”»åƒæ¨¡å‹">ç”»åƒæ¨¡å‹</h2>
  </li>
</ol>

<h1 id="å˜é‡ç­›é€‰çš„æŠ€æœ¯æŒ‡æ ‡">å˜é‡ç­›é€‰çš„æŠ€æœ¯æŒ‡æ ‡</h1>

<p>åŸºäºç¼ºå¤±ç‡ (Missing Rate)</p>

<p>åŸºäºå˜å¼‚ç³»æ•° (Coefficient of Variationï¼ŒCV)</p>

<p>åŸºäºç¨³å®šæ€§ (Population Stability Indexï¼ŒPSI)</p>

<p>åŸºäºä¿¡æ¯é‡ (Information Valueï¼ŒIV)</p>

<p>åŸºäºRF/XGBoostç‰¹å¾é‡è¦æ€§ (Feature Importance)</p>

<p>åŸºäºçº¿æ€§ç›¸å…³æ€§ (Linear Correlation)</p>

<p>åŸºäºå¤šé‡å…±çº¿æ€§ (Multicollinearity)</p>

<p>åŸºäºé€æ­¥å›å½’ (stepwise)</p>

<p>åŸºäºP-Vauleæ˜¾è‘—æ€§æ£€éªŒ</p>

<h1 id="è¯„åˆ†å¡è¯„ä¼°">è¯„åˆ†å¡è¯„ä¼°</h1>

<h2 id="åŒºåˆ†åº¦">åŒºåˆ†åº¦</h2>
<ul>
  <li>Gini</li>
  <li>AUC</li>
  <li>KS (Kolmogorov-Smirnov)</li>
</ul>

<h2 id="ç¨³å®šæ€§">ç¨³å®šæ€§</h2>
<p>PSI (ç¾¤ä½“ç¨³å®šæ€§æŒ‡æ ‡)</p>

<h2 id="æ’åºæ€§-ranking">æ’åºæ€§ (Ranking)</h2>
<p>ç»Ÿè®¡bad_rateã€liftã€Oddsç­‰æŒ‡æ ‡ï¼ŒæŒ‰ç…§è¯„åˆ†åˆ†æ•°è¿›è¡Œåˆ†ç®±ã€‚</p>

<h2 id="æ‹Ÿåˆåº¦-goodness-of-fit">æ‹Ÿåˆåº¦ (Goodness of Fit)</h2>

<h1 id="å…¶ä»–">å…¶ä»–</h1>

<h2 id="å¡æ–¹åˆ†ç®±-æœ‰ç›‘ç£åˆ†ç®±">å¡æ–¹åˆ†ç®± (æœ‰ç›‘ç£åˆ†ç®±)</h2>
<p>å¡æ–¹æ£€éªŒå°±æ˜¯å¯¹åˆ†ç±»æ•°æ®çš„é¢‘æ•°è¿›è¡Œåˆ†æçš„ä¸€ç§æ–¹æ³•ï¼Œå®ƒçš„åº”ç”¨ä¸»è¦è¡¨ç°åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š<strong>æ‹Ÿåˆä¼˜åº¦æ£€éªŒ</strong>å’Œ<strong>ç‹¬ç«‹æ€§æ£€éªŒï¼ˆåˆ—è”åˆ†æï¼‰</strong>ã€‚</p>
<h3 id="æ‹Ÿåˆä¼˜åº¦">æ‹Ÿåˆä¼˜åº¦</h3>

<p>æ‹Ÿåˆä¼˜åº¦æ˜¯å¯¹ä¸€ä¸ªåˆ†ç±»å˜é‡çš„æ£€éªŒï¼Œå³æ ¹æ®æ€»ä½“åˆ†å¸ƒçŠ¶å†µï¼Œè®¡ç®—å‡ºåˆ†ç±»å˜é‡ä¸­å„ç±»åˆ«çš„æœŸæœ›é¢‘æ•°ï¼Œä¸åˆ†å¸ƒçš„è§‚å¯Ÿé¢‘æ•°è¿›è¡Œå¯¹æ¯”ï¼Œåˆ¤æ–­æœŸæœ›é¢‘æ•°ä¸è§‚å¯Ÿé¢‘æ•°æ˜¯å¦æœ‰æ˜¾è‘—å·®å¼‚ï¼Œä»è€Œè¾¾åˆ°å¯¹åˆ†ç±»å˜é‡è¿›è¡Œåˆ†æçš„ç›®çš„ã€‚æ¯”å¦‚ï¼Œæ³°å¦å°¼å…‹å·ä¸­æˆ‘ä»¬è§‚å¯Ÿå¹¸å­˜è€…æ˜¯å¦ä¸æ€§åˆ«æœ‰å…³ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªXæ˜¯å¦ä¸Yæœ‰å¿…ç„¶è”ç³»ã€‚</p>

<h3 id="ç‹¬ç«‹æ€§æ£€éªŒ">ç‹¬ç«‹æ€§æ£€éªŒ</h3>

<p>ç‹¬ç«‹æ€§æ£€éªŒæ˜¯ä¸¤ä¸ªç‰¹å¾å˜é‡ä¹‹é—´çš„è®¡ç®—ï¼Œå®ƒå¯ä»¥ç”¨æ¥åˆ†æä¸¤ä¸ªåˆ†ç±»å˜é‡æ˜¯å¦ç‹¬ç«‹ï¼Œæˆ–è€…æ˜¯å¦æœ‰å…³è”ã€‚æ¯”å¦‚æŸåŸæ–™è´¨é‡å’Œäº§åœ°æ˜¯å¦ä¾èµ–å…³ç³»ï¼Œå¯ä»¥ç†è§£ä¸ºä¸€ä¸ªXä¸å¦ä¸€ä¸ªXæ˜¯å¦ç‹¬ç«‹ã€‚</p>

<h3 id="å¡æ–¹æ£€éªŒæ­¥éª¤">å¡æ–¹æ£€éªŒæ­¥éª¤</h3>

<p>(å¡æ–¹æ£€éªŒçš„æ­¥éª¤å…¶å®å°±æ˜¯ä¸€èˆ¬å‡è®¾æ£€éªŒçš„è¿‡ç¨‹)</p>

<ol>
  <li>æå‡ºå‡è®¾ï¼Œæ¯”å¦‚å‡è®¾ä¸¤ä¸ªå˜é‡ä¹‹é—´ç‹¬ç«‹</li>
  <li>æ ¹æ®åˆ†ç±»çš„è§‚å¯Ÿé¢‘æ•°è®¡ç®—æœŸæœ›é¢‘æ•°</li>
  <li>æ ¹æ®å¡æ–¹å…¬å¼ï¼Œè®¡ç®—å®é™…é¢‘æ•°ä¸æœŸæœ›é¢‘æ•°çš„å¡æ–¹å€¼</li>
  <li>æ ¹æ®è‡ªç”±åº¦å’Œäº‹å…ˆç¡®å®šçš„æ˜¾è‘—æ€§æ°´å¹³ï¼ŒæŸ¥æ‰¾å¡æ–¹åˆ†å¸ƒè¡¨è®¡ç®—å¡æ³•å€¼ï¼Œå¹¶ä¸ä¸Šä¸€æ­¥å¡æ–¹å€¼æ¯”è¾ƒ</li>
  <li>å¾—å‡ºç»“æœåˆ¤æ–­æ˜¯å¦æ‹’ç»åŸå‡è®¾</li>
</ol>

<p>å¡æ–¹åˆ†ç®±æ˜¯åŸºäº<strong>ç‹¬ç«‹æ€§æ£€éªŒ</strong>çš„åº”ç”¨ã€‚</p>

<h3 id="å¡æ–¹åˆ†ç®±æ­¥éª¤">å¡æ–¹åˆ†ç®±æ­¥éª¤</h3>
<ul>
  <li>åˆå§‹åŒ–</li>
</ul>

<p>æ ¹æ®è¿ç»­å˜é‡å€¼å¤§å°è¿›è¡Œæ’åº
æ„å»ºæœ€åˆçš„ç¦»æ•£åŒ–ï¼Œå³æŠŠæ¯ä¸€ä¸ªå•ç‹¬çš„å€¼è§†ä¸ºä¸€ä¸ªç®±ä½“ã€‚è¿™æ ·åšçš„ç›®çš„å°±æ˜¯æƒ³ä»æ¯ä¸ªå•ç‹¬çš„ä¸ªä½“å¼€å§‹é€æ¸åˆå¹¶ã€‚</p>
<ul>
  <li>åˆå¹¶
    <ul>
      <li>è®¡ç®—æ‰€æœ‰ç›¸é‚»åˆ†ç®±çš„å¡æ–¹å€¼ï¼šä¹Ÿå°±æ˜¯è¯´å¦‚æœæœ‰1,2,3,4ä¸ªåˆ†ç®±ï¼Œé‚£ä¹ˆå°±éœ€è¦ç»‘å®šç›¸é‚»çš„ä¸¤ä¸ªåˆ†ç®±ï¼Œå…±ä¸‰ç»„ï¼š12,23,34ã€‚ç„¶ååˆ†åˆ«è®¡ç®—ä¸‰ä¸ªç»‘å®šç»„çš„å¡æ–¹å€¼ã€‚</li>
      <li>ä»è®¡ç®—çš„å¡æ–¹å€¼ä¸­æ‰¾å‡ºæœ€å°çš„ä¸€ä¸ªï¼Œå¹¶æŠŠè¿™ä¸¤ä¸ªåˆ†ç®±åˆå¹¶ï¼šæ¯”å¦‚ï¼Œ23æ˜¯å¡æ–¹å€¼æœ€å°çš„ä¸€ä¸ªï¼Œé‚£ä¹ˆå°±å°†2å’Œ3åˆå¹¶ï¼Œæœ¬è½®è®¡ç®—ä¸­åˆ†ç®±å°±å˜ä¸ºäº†1,23,4ã€‚</li>
    </ul>

    <p><strong>åŸºæœ¬æ€æƒ³</strong>ï¼šå¦‚æœä¸¤ä¸ªç›¸é‚»çš„åŒºé—´å…·æœ‰éå¸¸ç±»ä¼¼çš„ç±»åˆ†å¸ƒï¼Œé‚£ä¹ˆè¿™ä¸¤ä¸ªåŒºé—´å¯ä»¥åˆå¹¶ã€‚å¦åˆ™ï¼Œå®ƒä»¬åº”è¯¥åˆ†å¼€ã€‚ä½å¡æ–¹å€¼è¡¨æ˜å®ƒä»¬å…·æœ‰ç›¸ä¼¼çš„ç±»åˆ†å¸ƒã€‚</p>
  </li>
  <li>
    <p>åœæ­¢æ¡ä»¶</p>

    <ul>
      <li>å¡æ–¹åœæ­¢çš„é˜ˆå€¼</li>
      <li>åˆ†ç®±æ•°ç›®çš„é™åˆ¶</li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<p><a href="https://zhuanlan.zhihu.com/p/370534836">æ¼«è°ˆä¿¡è´·ä¸šåŠ¡æ¨¡å‹ä½“ç³»å»ºè®¾</a> <br />
<a href="https://falbang.com/?p=350">ä¸€æ–‡çœ‹æ‡‚é£æ§æ¨¡å‹çš„æ‰€æœ‰ï¼ˆåº”è¯¥ï¼‰</a><br />
<a href="https://developer.kingdee.com/article/289708643615246080?productLineId=29&amp;isKnowledge=2&amp;lang=zh-CN&amp;islogin=true,true&amp;global=1">ä¸€æ–‡å¸¦ä½ äº†è§£é£æ§è¯„åˆ†å¡æ¨¡å‹</a> (å„ç§æŒ‡æ ‡åç§°)<br />
<a href="https://cloud.tencent.com/developer/article/1947627?areaId=106001">ç‰¹å¾ç¨³å®šæ€§æŒ‡æ ‡PSIçš„åŸç†ä¸ä»£ç åˆ†äº«</a><br />
<a href="https://cloud.tencent.com/developer/article/1530232">ä¸€æ–‡ä»‹ç»ç‰¹å¾å·¥ç¨‹é‡Œçš„å¡æ–¹åˆ†ç®±ï¼Œé™„ä»£ç å®ç°</a><br />
<a href="https://blog.csdn.net/richardsz_/article/details/123777141">ç‰¹å¾å·¥ç¨‹ | ä¿¡æ¯ä»·å€¼IVä¸ç¾¤ä½“ç¨³å®šæ€§PSI</a>
<a href="https://zhuanlan.zhihu.com/p/80134853">é£æ§æ¨¡å‹â€”WOEä¸IVæŒ‡æ ‡çš„æ·±å…¥ç†è§£åº”ç”¨</a> (è´å¶æ–¯è§’åº¦)<br />
<a href="https://www.zhihu.com/tardis/zm/art/143472559?source_id=1005">2.2ä¸‡å­—ï¼Œä¸€æ–‡çœ‹æ‡‚é£æ§æ¨¡å‹æ‰€æœ‰</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="financial" /><category term="risk management" /><category term="learning notes" /><summary type="html"><![CDATA[ä¿¡è´·ä¸šåŠ¡æ¨¡å‹ç»´åº¦]]></summary></entry><entry><title type="html">Fully Convolutional Networks</title><link href="http://localhost:4000/posts/2024/04/fcn" rel="alternate" type="text/html" title="Fully Convolutional Networks" /><published>2024-04-01T00:00:00+08:00</published><updated>2024-04-01T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/04/FCN</id><content type="html" xml:base="http://localhost:4000/posts/2024/04/fcn"><![CDATA[<p><a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a><br />
<a href="https://github.com/shelhamer/fcn.berkeleyvision.org">GitHub</a></p>
<ol>
  <li>
    <p>å»é™¤å…¨è¿æ¥å±‚ï¼š
ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰é€šå¸¸åŒ…å«å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…è®¤è¯†åˆ°å…¨è¿æ¥å±‚ä¼šå¯¼è‡´è¾“å‡ºå›ºå®šå¤§å°çš„ç‰¹å¾å‘é‡ï¼Œä»è€Œé™åˆ¶äº†CNNåœ¨å›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚
å› æ­¤ï¼Œä½œè€…å»é™¤äº†CNNä¸­çš„å…¨è¿æ¥å±‚ï¼Œå°†å…¶æ›¿æ¢ä¸ºå…¨å·ç§¯å±‚ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿæ¥å—ä»»æ„å¤§å°çš„è¾“å…¥å›¾åƒï¼Œå¹¶è¾“å‡ºç›¸åŒå°ºå¯¸çš„åƒç´ çº§åˆ«çš„åˆ†ç±»ç»“æœã€‚</p>
  </li>
  <li>
    <p>è½¬æ¢ä¸ºå…¨å·ç§¯ç½‘ç»œï¼š
é€šè¿‡å°†CNNä¸­çš„å…¨è¿æ¥å±‚æ›¿æ¢ä¸ºå…¨å·ç§¯å±‚ï¼Œä½œè€…å°†CNNè½¬æ¢ä¸ºäº†å…¨å·ç§¯ç½‘ç»œï¼ˆFCNï¼‰ã€‚
å…¨å·ç§¯ç½‘ç»œå…·æœ‰ç«¯åˆ°ç«¯çš„ç‰¹æ€§ï¼Œèƒ½å¤Ÿç›´æ¥å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåƒç´ çº§åˆ«çš„åˆ†ç±»ï¼Œä»è€Œå®ç°äº†å›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡çš„ç«¯åˆ°ç«¯æ˜ å°„ã€‚</p>
  </li>
  <li>
    <p>ä½¿ç”¨åå·ç§¯ï¼ˆè½¬ç½®å·ç§¯ï¼‰è¿›è¡Œä¸Šé‡‡æ ·ï¼š
åœ¨FCNä¸­ï¼Œä¸ºäº†å°†å·ç§¯å±‚çš„è¾“å‡ºæ¢å¤åˆ°è¾“å…¥å›¾åƒç›¸åŒçš„å¤§å°ï¼Œä½œè€…ä½¿ç”¨äº†åå·ç§¯ï¼ˆdeconvolutionï¼‰è¿›è¡Œä¸Šé‡‡æ ·ã€‚
é€šè¿‡åå·ç§¯æ“ä½œï¼Œç½‘ç»œèƒ½å¤Ÿå­¦ä¹ å°†ä½åˆ†è¾¨ç‡çš„ç‰¹å¾å›¾æ‰©å±•åˆ°ä¸è¾“å…¥å›¾åƒç›¸åŒçš„åˆ†è¾¨ç‡ï¼Œä»è€Œå¾—åˆ°åƒç´ çº§åˆ«çš„åˆ†ç±»ç»“æœã€‚</p>
  </li>
  <li>
    <p>æ·»åŠ è·³è·ƒè¿æ¥ï¼š
ä¸ºäº†æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½œè€…åœ¨FCNä¸­å¼•å…¥äº†è·³è·ƒè¿æ¥ï¼ˆskip connectionsï¼‰ã€‚
è·³è·ƒè¿æ¥èƒ½å¤Ÿå°†ä¸åŒå±‚æ¬¡çš„è¯­ä¹‰ä¿¡æ¯ç»“åˆèµ·æ¥ï¼Œä»è€Œæé«˜äº†ç½‘ç»œå¯¹å±€éƒ¨å’Œå…¨å±€ä¿¡æ¯çš„æ„ŸçŸ¥èƒ½åŠ›ï¼Œè¿›è€Œæé«˜äº†åˆ†å‰²ç²¾åº¦ã€‚</p>
  </li>
  <li>
    <p>ä½¿ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆï¼š
ä¸ºäº†è¿›ä¸€æ­¥æé«˜åˆ†å‰²ç²¾åº¦ï¼Œä½œè€…è¿˜æå‡ºäº†ä½¿ç”¨å¤šå°ºåº¦ç‰¹å¾èåˆçš„æ–¹æ³•ã€‚
é€šè¿‡å°†ä¸åŒå±‚æ¬¡çš„ç‰¹å¾å›¾è¿›è¡Œèåˆï¼Œç½‘ç»œèƒ½å¤ŸåŒæ—¶å…³æ³¨ä¸åŒå°ºåº¦çš„ä¿¡æ¯ï¼Œä»è€Œæé«˜äº†å¯¹å›¾åƒè¯­ä¹‰çš„ç†è§£èƒ½åŠ›ã€‚</p>
  </li>
</ol>

<p><a href="https://mp.weixin.qq.com/s/UbGYZFsJsgBZgtH0-9gL7Q">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="deep learning" /><category term="computer vision" /><category term="learning notes" /><summary type="html"><![CDATA[Fully Convolutional Networks for Semantic Segmentation GitHub å»é™¤å…¨è¿æ¥å±‚ï¼š ä¼ ç»Ÿçš„å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰é€šå¸¸åŒ…å«å·ç§¯å±‚å’Œå…¨è¿æ¥å±‚ã€‚åœ¨è¿™ç¯‡è®ºæ–‡ä¸­ï¼Œä½œè€…è®¤è¯†åˆ°å…¨è¿æ¥å±‚ä¼šå¯¼è‡´è¾“å‡ºå›ºå®šå¤§å°çš„ç‰¹å¾å‘é‡ï¼Œä»è€Œé™åˆ¶äº†CNNåœ¨å›¾åƒè¯­ä¹‰åˆ†å‰²ä»»åŠ¡ä¸­çš„åº”ç”¨ã€‚ å› æ­¤ï¼Œä½œè€…å»é™¤äº†CNNä¸­çš„å…¨è¿æ¥å±‚ï¼Œå°†å…¶æ›¿æ¢ä¸ºå…¨å·ç§¯å±‚ï¼Œä½¿å¾—ç½‘ç»œèƒ½å¤Ÿæ¥å—ä»»æ„å¤§å°çš„è¾“å…¥å›¾åƒï¼Œå¹¶è¾“å‡ºç›¸åŒå°ºå¯¸çš„åƒç´ çº§åˆ«çš„åˆ†ç±»ç»“æœã€‚]]></summary></entry><entry><title type="html">Convolutional Neural Networks</title><link href="http://localhost:4000/posts/2024/03/cnn" rel="alternate" type="text/html" title="Convolutional Neural Networks" /><published>2024-03-31T00:00:00+08:00</published><updated>2024-03-31T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/CNN</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/cnn"><![CDATA[<p>ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Ÿå®ƒåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ</p>

<p>å·ç§¯ç¥ç»ç½‘ç»œï¼ˆConvolutional Neural Networkï¼ŒCNNï¼‰æ˜¯ä¸€ç§æ·±åº¦å­¦ä¹ æ¨¡å‹ï¼Œç‰¹åˆ«é€‚ç”¨äºå¤„ç†å…·æœ‰ç½‘æ ¼ç»“æ„çš„æ•°æ®ï¼Œå¦‚å›¾åƒå’Œè§†é¢‘ã€‚
CNNçš„æ ¸å¿ƒæ€æƒ³æ˜¯é€šè¿‡å·ç§¯æ“ä½œå’Œæ± åŒ–æ“ä½œæ¥æå–è¾“å…¥æ•°æ®çš„ç‰¹å¾ï¼Œå¹¶é€šè¿‡è¿™äº›ç‰¹å¾è¿›è¡Œåˆ†ç±»ã€è¯†åˆ«æˆ–å›å½’ç­‰ä»»åŠ¡ã€‚</p>

<p>åŸºæœ¬åŸç†</p>
<ul>
  <li>
    <p>å·ç§¯å±‚ï¼ˆConvolutional Layerï¼‰ï¼šå·ç§¯æ“ä½œæ˜¯CNNä¸­çš„æ ¸å¿ƒéƒ¨åˆ†ã€‚å®ƒä½¿ç”¨ä¸€ä¸ªå·ç§¯æ ¸ï¼ˆfilterï¼‰åœ¨è¾“å…¥æ•°æ®ä¸Šæ»‘åŠ¨ï¼Œå°†å·ç§¯æ ¸ä¸è¾“å…¥æ•°æ®çš„å±€éƒ¨åŒºåŸŸè¿›è¡Œå¯¹åº”å…ƒç´ ç›¸ä¹˜å¹¶æ±‚å’Œï¼Œç”Ÿæˆè¾“å‡ºç‰¹å¾å›¾ã€‚è¿™ä¸€æ“ä½œèƒ½å¤Ÿæå–å‡ºè¾“å…¥æ•°æ®çš„å±€éƒ¨ç‰¹å¾ã€‚</p>
  </li>
  <li>
    <p>æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰ï¼šé€šå¸¸åœ¨å·ç§¯å±‚ä¹‹åä¼šæ·»åŠ ä¸€ä¸ªéçº¿æ€§çš„æ¿€æ´»å‡½æ•°ï¼Œå¦‚ReLUï¼ˆRectified Linear Unitï¼‰ï¼Œç”¨äºå¼•å…¥éçº¿æ€§ï¼Œå¢å¼ºç½‘ç»œçš„è¡¨è¾¾èƒ½åŠ›ã€‚</p>
  </li>
  <li>
    <p>æ± åŒ–å±‚ï¼ˆPooling Layerï¼‰ï¼šæ± åŒ–æ“ä½œç”¨äºå‡å°ç‰¹å¾å›¾çš„å°ºå¯¸å¹¶ä¿ç•™å…¶é‡è¦ä¿¡æ¯ï¼Œå¸¸è§çš„æ± åŒ–æ“ä½œæœ‰æœ€å¤§æ± åŒ–ï¼ˆMax Poolingï¼‰å’Œå¹³å‡æ± åŒ–ï¼ˆAverage Poolingï¼‰ç­‰ã€‚</p>
  </li>
  <li>
    <p>å…¨è¿æ¥å±‚ï¼ˆFully Connected Layerï¼‰ï¼šåœ¨å·ç§¯å±‚å’Œæ± åŒ–å±‚ä¹‹åï¼Œé€šå¸¸ä¼šè¿æ¥ä¸€ä¸ªæˆ–å¤šä¸ªå…¨è¿æ¥å±‚ï¼Œç”¨äºå°†ç‰¹å¾å›¾å±•å¹³å¹¶è¿æ¥åˆ°è¾“å‡ºå±‚ï¼Œä»¥è¿›è¡Œæœ€ç»ˆçš„åˆ†ç±»æˆ–å›å½’ã€‚</p>
  </li>
</ul>

<p><a href="https://mp.weixin.qq.com/s/-Qejp91LOImLXZg0VluuMA">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="deep learning" /><category term="computer vision" /><category term="learning notes" /><summary type="html"><![CDATA[ä»€ä¹ˆæ˜¯å·ç§¯ç¥ç»ç½‘ç»œï¼ˆCNNï¼‰ï¼Ÿå®ƒåœ¨è®¡ç®—æœºè§†è§‰ä¸­çš„åº”ç”¨æ˜¯ä»€ä¹ˆï¼Ÿ]]></summary></entry><entry><title type="html">Basic pandas</title><link href="http://localhost:4000/posts/2034/03/basic-pandas/" rel="alternate" type="text/html" title="Basic pandas" /><published>2024-03-22T00:00:00+08:00</published><updated>2024-03-22T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/03/pandas</id><content type="html" xml:base="http://localhost:4000/posts/2034/03/basic-pandas/"><![CDATA[<p>Python, data science, pandas</p>

<p><a href="https://mp.weixin.qq.com/s/0WEoyDAN39KtbhIrgCImeA">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="python" /><category term="data science" /><summary type="html"><![CDATA[Python, data science, pandas]]></summary></entry><entry><title type="html">Basic numpy</title><link href="http://localhost:4000/posts/2034/03/basic-numpy/" rel="alternate" type="text/html" title="Basic numpy" /><published>2024-03-22T00:00:00+08:00</published><updated>2024-03-22T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/03/numpy</id><content type="html" xml:base="http://localhost:4000/posts/2034/03/basic-numpy/"><![CDATA[<p>Python, data science, numpy</p>

<p><a href="https://mp.weixin.qq.com/s/zgcugVeBE2grz9rwqPm8Ig">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="python" /><category term="data science" /><summary type="html"><![CDATA[Python, data science, numpy]]></summary></entry><entry><title type="html">Happy 31th birthday</title><link href="http://localhost:4000/posts/2024/03/31th-birthday/" rel="alternate" type="text/html" title="Happy 31th birthday" /><published>2024-03-15T00:00:00+08:00</published><updated>2024-03-15T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/31th</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/31th-birthday/"><![CDATA[<p>ä»Šå¤©ä¸è°ˆæœºå™¨å­¦ä¹ ï¼Œä¸å†æ•´ç†ä¸€äº›è®°ä¸ä½çš„çŸ¥è¯†ï¼Œè€Œæ˜¯åœ¨ä¸‰ååˆä¸€çš„å½“ä¸‹ï¼Œè®°å½•ä¸€ä¸‹æœ€è¿‘çš„æ€è€ƒã€‚</p>

<p>ä¸ºä»€ä¹ˆæœ€è¿‘ä¼šæœ‰æ—¶é—´åšè¿™ä¹ˆå¤šæ€è€ƒå‘¢ï¼Ÿå› ä¸ºæˆ‘åœ¨ä»Šå¹´1æœˆï¼Œå†œå†æ–°å¹´å‰ä¸åˆ°1ä¸ªæœˆçš„æ—¶å€™ï¼Œè¯•ç”¨æœŸè¿˜æœ‰1å‘¨å°±è½¬æ­£çš„æ—¶å€™ï¼Œçªç„¶è¢«é€šçŸ¥ä¸è½¬æ­£äº†ã€‚</p>

<p>åˆ°ç°åœ¨åº”è¯¥å¤±ä¸šæ­£å¥½å·®ä¸å¤šä¸¤ä¸ªæœˆï¼Œä¸­é—´å†œå†æ–°å¹´æœŸé—´å®Œå…¨æ˜¯ä¸€ä¸ªæ‹›è˜åœæ»çš„çŠ¶æ€ã€‚ç„¶ååœ¨æˆ‘31å²ç”Ÿæ—¥ä¹‹å‰è‡³å°‘æœ‰ä¸€ä¸ªæ¯”è¾ƒè¿‡å¾—å»çš„å£å¤´offerï¼Œä¹Ÿç®—ä¸é‚£ä¹ˆç„¦è™‘äº†ã€‚</p>

<p>å¤ç›˜ä»è¢«è§£é›‡åˆ°ç°åœ¨ï¼Œæˆ‘çš„å¿ƒè·¯å†ç¨‹ä¸€ç›´éå¸¸æ¸…æ™°ã€‚æœ€å¼€å§‹å°å¿ƒç¿¼ç¿¼é™ä½å§¿æ€è·Ÿå…¬å¸äº‰å–æœºä¼šï¼Œæ‰¿è®¤è‡ªå·±æœ‰ä¸è¶³ä¹‹å¤„ï¼ˆè¿™ç‚¹æˆ‘ç°åœ¨ä¹Ÿä¸å¦è®¤ï¼Œä½†æ˜¯ï¼Œäººæ— å®Œäººï¼‰ï¼›
åæ¥äº†è§£åˆ°åº”è¯¥å»æ‰¾å‰å…¬å¸ä»²è£ï¼Œä½†æ˜¯å› ä¸ºé«˜æ˜‚å¾‹å¸ˆè´¹è®¡åˆ’è¢«æç½®ï¼ˆä½†æˆ‘æ²¡æœ‰æ”¾å¼ƒè¿™ä¸ªæƒ³æ³•ï¼Œæ¯•ç«Ÿä»²è£æ˜¯ä¸€å¹´å†…éƒ½æœ‰æ•ˆçš„ï¼Œå¿ƒé‡Œå­˜ç€ä¸€å£æ°”ï¼‰ï¼›</p>

<p>å†åæ¥è·ŸåŒæœŸä¸»åŠ¨ç¦»èŒçš„åŒäº‹å–å¾—è”ç³»ï¼ˆå› ä¸ºä¸å±äºä¸€ä¸ªéƒ¨é—¨ï¼Œä¹‹å‰å·¥ä½œæ²¡æœ‰äº¤é›†ï¼‰ï¼Œè®¤è¯†åˆ°å…¬å¸çš„é—®é¢˜è¿œå¤§äºè‡ªå·±çš„é—®é¢˜ï¼Œä¸åº”è¯¥å†…è€—ï¼ˆè¿˜æœ‰å¾ˆé‡è¦çš„ï¼Œä»–å‘Šè¯‰æˆ‘ä¸è¦è®©åˆ«äººè¯„åˆ¤è‡ªå·±æœ‰æ²¡æœ‰è¿›æ­¥ï¼Œè‡ªå·±çŸ¥é“è‡ªå·±è¿™å‡ ä¸ªæœˆæ˜¯æœ‰è¿›æ­¥çš„ã€æ˜¯è·Ÿä¹‹å‰çš„è‡ªå·±ä¸ä¸€æ ·çš„ï¼Œå°±å¯ä»¥äº†ï¼‰ï¼›</p>

<p>å†æ¥ç€æ˜¯å›é¡¾åœ¨èŒæœŸé—´ï¼Œæ¯å¤©è¶…æ—¶å·¥ä½œæ²¡æœ‰ä»€ä¹ˆåˆ«çš„ç”Ÿæ´»ï¼Œä½†æ˜¯å…¬å¸è¿˜æ˜¯ä¸æ»¡æ„ï¼Œå½“æ—¶æ•´ä¸ªäººä¹Ÿä¸¥é‡æŠ‘éƒï¼Œåªä¸è¿‡å½“ä¸‹è¢«å·¥ä½œçš„å¿™ç¢Œæ‰€å¡«æ»¡ï¼Œæ²¡æœ‰åŠæ—¶åæ€ï¼Œä¸ç„¶å¯èƒ½åƒå‰åŒäº‹ä¸€æ ·ä¹Ÿåº”è¯¥ä¸»åŠ¨ç¦»èŒäº†ï¼Œæ‰€ä»¥ä¹Ÿæ˜¯å¥½äº‹ï¼Œä½†æ˜¯ç‹—å…¬å¸è¿™ç§åšæ³•çœŸçš„è¿‡äºæ¶å¿ƒï¼›
å†æ¥ä¸‹æ¥å°±æ˜¯ç§¯ææ‰¾å·¥ä½œï¼Œä½†æ˜¯è¿™ä¸ªè¿‡ç¨‹æ— ç–‘æ˜¯ä¸‡åˆ†ç—›è‹¦åŠ ä¸Šå……æ»¡äº†è‡ªæˆ‘æ€€ç–‘ï¼Œå°±å› ä¸ºâ€œè¯•ç”¨æœŸ<strong>é©¬ä¸Šè½¬æ­£</strong>è¢«è¾é€€â€è¿™ä¸€ç‚¹ï¼Œå°±è¢«åå¤è´¨ç–‘æˆ‘çš„å·¥ä½œèƒ½åŠ›ã€‚</p>

<p>å¯¹å‰å…¬å¸å¯ä»¥è¯´å……æ»¡äº†æ¨ï¼Œè®©æˆ‘åœ¨æˆ‘å›å›½çš„ç¬¬ä¸€å¹´ï¼ŒåŠ å…¥çš„ç¬¬ä¸€å®¶å…¬å¸å°±æ¥åˆ°å¦‚æ­¤å¤§ç¤¼ï¼Œæˆ‘çœŸæ˜¯ä¸‡åˆ†æ„Ÿæ¿€ğŸ™<br />
è®©æˆ‘åœ¨èŒæœŸé—´å°±ä¸æ–­å†…è€—ï¼Œç»“æœç”¨å®Œå°±æ‰”<br />
å“¦å¯¹ï¼Œé™¤äº†ç”¨å®Œå°±æ‰”ï¼Œåœ¨â€œé€šçŸ¥â€æˆ‘çš„æ—¶å€™è¿˜ä¸å¿˜è®½åˆºæ¶å¿ƒæˆ‘ï¼Œå¯æ˜¯æ˜æ˜å°±æ˜¯å…¬å¸è‡ªå·±ä¸è¡Œå•Šï¼Œå°±æ˜¯æ‰€è°“â€œé¢†å¯¼è‡ªå·±èƒ½åŠ›ä¸è¡Œè®©ä¸‹å±èƒŒé”…â€<br />
è®©æˆ‘è¿ç»­ä¸‰å¤©æ²¡æœ‰ä¸‹åºŠï¼Œé†’äº†å°±å“­<br />
è®©æˆ‘åœ¨å†œå†æ–°å¹´å‰è¿·èŒ«ï¼Œæå‰å›å®¶è¿˜è¦è·Ÿæˆ‘å¦ˆå‡è£…åœ¨è¿œç¨‹åŠå…¬<br />
è¿™ä»¶äº‹åªè·ŸRCè¯´äº†ï¼ŒåæœŸè¿˜è·Ÿä¸€ä¸ªå…³ç³»è¿‘çš„å‰è¾ˆï¼ˆè¡¨å“¥ï¼‰è¯´äº†ï¼Œå¯¼è‡´æˆ‘è¿™æ®µæ—¶é—´ä¹Ÿä¸æ•¢å»è”ç³»ä»»ä½•æœ‹å‹ï¼ˆè™½ç„¶RCè¯´ï¼Œæœ‹å‹å¹¶ä¸æ˜¯åªæœ‰é£å…‰çš„æ—¶å€™æ‰èƒ½è”ç³»ï¼Œä½†æˆ‘å½“ä¸‹çš„çŠ¶æ€å°±æ˜¯å‡†å¤‡é¢è¯•ä¸ç„¦è™‘ï¼Œå¯èƒ½çœŸçš„å¾ˆéš¾è·Ÿæœ‹å‹æ­£å¸¸äº¤è°ˆï¼‰</p>

<p>ä½†æ˜¯å¤±ä¸šè¿™æ®µæ—¶é—´è®©æˆ‘æŠŠæˆ‘å¥½ä¹…æ²¡æœ‰æ›´æ–°çš„ç½‘ç«™ç»™æ›´æ–°äº†ï¼Œè¿˜å¼€å§‹å†™åšå®¢ï¼ˆä¸»è¦éƒ½æ˜¯ä¸ºäº†è‡ªå·±å¤ä¹ å‡†å¤‡é¢è¯•ï¼‰ã€‚æ¬å®¶åæ¯”è¾ƒé€‚åˆæˆ·å¤–è·‘æ­¥ï¼Œä¹Ÿå·²ç»è¿ç»­2å‘¨ï¼ˆå‡ ä¹ï¼‰æ¯å¤©æ—©èµ·è·‘æ­¥äº†ã€‚ä¹Ÿæœ‰æ—¶é—´é˜…è¯»ä¸€äº›â€œéå°è¯´ç±»â€ä¹¦ç±ï¼ˆä½†æ˜¯è¯´çœŸçš„ï¼ŒçœŸçš„æ¯æ¬¡éƒ½æœ‰äº›æŠ—æ‹’å»è¯»ï¼Œå› ä¸ºçœŸçš„å¾ˆéš¾æ‡‚å•Šï¼Œæˆ‘è¯´çš„å°±æ˜¯ã€Šå¿ƒç†ç±»å‹ã€‹ï¼Œä¸è¿‡ã€Šé»‘å®¢ä¸ç”»å®¶ã€‹è¿˜ä¸é”™ï¼‰</p>

<p>å¹´çºªå¢é•¿ï¼Œæœ€è¿‘ä¹Ÿæ˜æ˜¾çœ‹åˆ°äº†è‡ªå·±èº«ä¸Šâ€œå²æœˆçš„ç—•è¿¹â€ï¼šé•¿èƒ–ã€å‘è…®ã€çœ¼çº¹ã€é¢ˆçº¹ã€ç—˜å°æ¶ˆä¸äº†ï¼ˆå¯¹å•Šæˆ‘å¯æ˜¯åˆ°30+è¿˜åœ¨ä¸€ç›´é•¿ç—˜å‘¢ï¼‰ï¼Œæ¶ˆåŒ–èƒ½åŠ›ä¹Ÿå·®â€¦â€¦è€Œä¸”æœ€è¿‘çš„çŠ¶æ€çœŸæ˜¯ä¸èƒ½è¯´â€œç¨³å®šâ€ï¼šå¥½å‡ æ¬¡æƒ³è¦è·³ä¸‹å»å°±å®Œäº†</p>

<p>æˆ‘ä¼šè¢«è´¨ç–‘ä¸ä¼šæ²Ÿé€šï¼Œä¼šè¢«è´¨ç–‘ç»™ä¸äº†æƒ…ç»ªä»·å€¼ï¼Œè¯´çœŸçš„ï¼Œé‚£æˆ‘è¿˜æœ‰ä»€ä¹ˆä»·å€¼å‘¢ï¼Ÿ</p>

<p>å¥½å§ï¼Œå†™åˆ°æœ€åæˆ‘æ‰æ‰¿è®¤ï¼Œè¿™å¹¶ä¸æ˜¯å¸¸è§çš„é‚£ç§å……æ»¡å¸Œæœ›ç§¯æå‘ä¸Šçš„30+å°å§å§å¯¹ç”Ÿæ´»çš„æ„Ÿæ‚Ÿ</p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="personal" /><summary type="html"><![CDATA[ä»Šå¤©ä¸è°ˆæœºå™¨å­¦ä¹ ï¼Œä¸å†æ•´ç†ä¸€äº›è®°ä¸ä½çš„çŸ¥è¯†ï¼Œè€Œæ˜¯åœ¨ä¸‰ååˆä¸€çš„å½“ä¸‹ï¼Œè®°å½•ä¸€ä¸‹æœ€è¿‘çš„æ€è€ƒã€‚]]></summary></entry><entry><title type="html">Clustering alrogithms</title><link href="http://localhost:4000/posts/2024/03/stochastic-gradient-descent/" rel="alternate" type="text/html" title="Clustering alrogithms" /><published>2024-03-14T00:00:00+08:00</published><updated>2024-03-14T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/gradient-descent</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/stochastic-gradient-descent/"><![CDATA[<p>gradient descent, stochastic gradient descent, batch gradient descent</p>

<h1 id="improvements-for-gradient-descent">Improvements for gradient descent</h1>
<p>Momentum (æƒ¯æ€§ä¿æŒ)
â€”â€”
\(v_t = \gamma v_{t-1}+ \eta g_t\)
\(\theta_{t+1} = \theta_{t}-v_t\)</p>

<ul>
  <li>å­¦ä¹ é€Ÿç‡$\eta$ä¹˜å½“å‰æ¢¯åº¦$g_t$.</li>
  <li>å¸¦è¡°å‡çš„å‰ä¸€æ¬¡çš„æ­¥ä¼$v_{t-1}$.</li>
</ul>

<h2 id="adagrad-ç¯å¢ƒæ„ŸçŸ¥">AdaGrad (ç¯å¢ƒæ„ŸçŸ¥)</h2>
<p>é‡‡ç”¨â€œå†å²æ¢¯åº¦å¹³æ–¹å’Œâ€æ¥è¡¡é‡ä¸åŒå‚æ•°çš„æ¢¯åº¦å’Œç¨€ç–æ€§ï¼ˆå–å€¼è¶Šå°è¶Šç¨€ç–ï¼‰</p>

<h2 id="adam-æƒ¯æ€§ä¿æŒç¯å¢ƒæ„ŸçŸ¥">Adam (æƒ¯æ€§ä¿æŒ+ç¯å¢ƒæ„ŸçŸ¥)</h2>
<ul>
  <li>æ¢¯åº¦çš„ä¸€é˜¶çŸ© (first moment)ï¼šè¿‡å¾€æ¢¯åº¦ä¸å½“å‰æ¢¯åº¦çš„å¹³å‡ -&gt; æƒ¯æ€§ä¿æŒ</li>
  <li>æ¢¯åº¦çš„äºŒé˜¶çŸ© (second moment)ï¼šè¿‡å¾€æ¢¯åº¦å¹³æ–¹ä¸å½“å‰æ¢¯åº¦å¹³æ–¹çš„å¹³å‡ -&gt; ç¯å¢ƒæ„ŸçŸ¥ï¼Œä¸ºä¸åŒå‚æ•°äº§ç”Ÿè‡ªé€‚åº”çš„å­¦ä¹ é€Ÿç‡</li>
</ul>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="learning notes" /><summary type="html"><![CDATA[gradient descent, stochastic gradient descent, batch gradient descent]]></summary></entry><entry><title type="html">Activation Functions</title><link href="http://localhost:4000/posts/2024/03/activation-functions/" rel="alternate" type="text/html" title="Activation Functions" /><published>2024-03-13T00:00:00+08:00</published><updated>2024-03-13T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/activation-functions</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/activation-functions/"><![CDATA[<p>æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰ï¼Œè´Ÿè´£å°†ç¥ç»å…ƒçš„è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºç«¯ï¼Œæ¿€æ´»å‡½æ•°å°†ç¥ç»ç½‘ç»œä¸­å°†è¾“å…¥ä¿¡å·çš„æ€»å’Œè½¬æ¢ä¸ºè¾“å‡ºä¿¡å·ã€‚æ¿€æ´»å‡½æ•°å¤§å¤šæ˜¯éçº¿æ€§å‡½æ•°ï¼Œæ‰èƒ½å°†å¤šå±‚æ„ŸçŸ¥æœºçš„è¾“å‡ºè½¬æ¢ä¸ºéçº¿æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä»»æ„é€¼è¿‘ä»»ä½•éçº¿æ€§å‡½æ•°ï¼Œè¿›è€Œå¯ä»¥åº”ç”¨åˆ°ä¼—å¤šçš„éçº¿æ€§æ¨¡å‹ä¸­ã€‚</p>

<h1 id="sigmoid-family">Sigmoid Family</h1>
<p>\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</p>

<h2 id="hard-sigmoid">Hard Sigmoid</h2>
<p>\(\)</p>

<h2 id="swish">Swish</h2>
<p>\(swish(x) = x\cdot sigmoid(x) = \frac{x}{1+e^{-x}}\)</p>

<h2 id="maxout">Maxout</h2>

<h1 id="relu-family-rectified-linear-unit">ReLU Family (Rectified Linear Unit)</h1>
<p>\(ReLU(x) = max(0,x)\)</p>
<ul>
  <li>Dying neuron</li>
  <li>Handles the vanishing gradient issue</li>
  <li>Cannot avoid exploding gradient issue</li>
</ul>

<h2 id="elu">ELU</h2>
<p>\(ELU(x) = \left\{
\begin{aligned}
x,\ if\ x\ \geq 0 \\
\alpha(e^x-1),\ x&lt;0
\end{aligned}
\right.\)</p>

<ul>
  <li>Avoids dying neuron issue</li>
  <li>Cannot avoid exploding gradient</li>
  <li>Computational expensive (because of exponantial calculation)</li>
  <li>$\alpha$ is an hyper-parameter (normally, $\alpha$ between 0.1 and 0.3)</li>
</ul>

<h2 id="leaky-relu">Leaky ReLU</h2>
<p>\(LeakyReLU(x) = \left\{
\begin{aligned}
x,\ if\ x\ \geq 0 \\
\alpha x,\ x&lt;0
\end{aligned}
\right.\)</p>

<ul>
  <li>Avoids dying neuron issue</li>
  <li>Not computational expensive</li>
  <li>$\alpha$ is an hyper-parameter (normally, $\alpha$ between 0.1 and 0.3)</li>
</ul>

<h2 id="selu-scaled-exponential-linear-units">SELU (Scaled Exponential Linear Units)</h2>
<p>\(SELU(x) = \left\{
\begin{aligned}
\lambda x,\ if\ x\ \geq 0 \\
\lambda \alpha (e^x-1),\ x&lt;0
\end{aligned}
\right.\)
$\alpha \approx 1.6733$, $\lambda \approx 1.0507$.</p>

<h2 id="gelu-gaussian-error-linear-unit">GELU (Gaussian Error Linear Unit)</h2>
<p>\(GELU(x) = xP(X\leq x) = x\Phi(x) = x\cdot\frac{1}{2}[1+erf(\frac{x}{\sqrt(2)})]\)
if $X \sim \mathcal{N}(0,1)$.</p>

<ul>
  <li>GELUs are used in GPT-3, BERT, and most other Transformers.</li>
</ul>

<h1 id="tanh-family">Tanh Family</h1>
<p>\(Tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</p>
<ul>
  <li>Vanishing gradient problem</li>
  <li>Symmetric about the origin</li>
</ul>

<h2 id="hardtanh">HardTanh</h2>
<p>\(HardTanh(x) = \left\{
\begin{aligned}
-1,\ if\ x &lt; -1\\
x,\ if\ -1\leq x\leq 0\\
1,\ if x&gt;1
\end{aligned}
\right.\)</p>

<ul>
  <li>It is a cheaper and more computationally efficient version of the tanh activation.</li>
</ul>

<h2 id="tanhshrink">TanhShrink</h2>

<h1 id="softmax">Softmax</h1>

<h2 id="logsoftmax">LogSoftMax</h2>

<h2 id="softmin">Softmin</h2>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="learning notes" /><summary type="html"><![CDATA[æ¿€æ´»å‡½æ•°ï¼ˆActivation Functionï¼‰ï¼Œè´Ÿè´£å°†ç¥ç»å…ƒçš„è¾“å…¥æ˜ å°„åˆ°è¾“å‡ºç«¯ï¼Œæ¿€æ´»å‡½æ•°å°†ç¥ç»ç½‘ç»œä¸­å°†è¾“å…¥ä¿¡å·çš„æ€»å’Œè½¬æ¢ä¸ºè¾“å‡ºä¿¡å·ã€‚æ¿€æ´»å‡½æ•°å¤§å¤šæ˜¯éçº¿æ€§å‡½æ•°ï¼Œæ‰èƒ½å°†å¤šå±‚æ„ŸçŸ¥æœºçš„è¾“å‡ºè½¬æ¢ä¸ºéçº¿æ€§ï¼Œä½¿å¾—ç¥ç»ç½‘ç»œå¯ä»¥ä»»æ„é€¼è¿‘ä»»ä½•éçº¿æ€§å‡½æ•°ï¼Œè¿›è€Œå¯ä»¥åº”ç”¨åˆ°ä¼—å¤šçš„éçº¿æ€§æ¨¡å‹ä¸­ã€‚]]></summary></entry><entry><title type="html">Loss Functions</title><link href="http://localhost:4000/posts/2024/03/loss-functions/" rel="alternate" type="text/html" title="Loss Functions" /><published>2024-03-12T00:00:00+08:00</published><updated>2024-03-12T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/loss-functions</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/loss-functions/"><![CDATA[<h1 id="regression-loss-functions">Regression Loss Functions</h1>

<h2 id="l1-loss-mean-absolute-error-mse">L1 Loss (Mean Absolute Error, MSE)</h2>
<p>\(MSE = \frac{\sum_{i=1}^{N}|f(x_{i})-y_i|}{N}\)</p>

<h2 id="l2-loss-mean-square-error-mse">L2 Loss (Mean Square Error, MSE)</h2>
<p>\(MSE = \frac{\sum_{i=1}^{N}(f(x_{i})-y_i)^2}{N}\)</p>

<h2 id="smooth-l1-loss">Smooth L1 Loss</h2>
<p>\(SmoothL1 = \left\{
\begin{aligned}
0.5x^2,\ if\ |x|\ &lt;\ 1 \\
|x-0.5|,\ otherwise
\end{aligned}
\right.\)</p>

<h2 id="huber-loss">Huber Loss</h2>
<p>Combines MSE and MAE.</p>

<h1 id="classification-loss-functions">Classification Loss Functions</h1>
<p>Cross Entropy Loss
â€”â€”</p>

<p>Binary case:
\(CE(p, y) = \left\{
  \begin{aligned}
  -log(p),\ if\ y\ =\ 1, \\
  -log(1-p),\ otherwise
  \end{aligned}
  \right.\)</p>

<p>\(p_t = \left\{
  \begin{aligned}
  p,\ if\ y=1,\\
  1-p,\ otherwise 
  \end{aligned}
\right.\)
Simplified to
\(CE(p,y) = CE(pt) = -log(pt)\)</p>

<p>Multi-class case:
\(CE(p_i, y_i) = -y_{i,c}log(p_{ic})\)</p>

\[L = \frac{1}{N}\sum_{i}L_i = -\frac{1}{N}\sum_{i}\sum_{c=1}^{M}y_{ic}log(p_{ic})\]

<h2 id="balanced-cross-entropy-loss">Balanced Cross Entropy Loss</h2>
<p>\(CE(p_t) = -\alpha_tlog(p_t)\)</p>

<p>Balanced cross entropy loss solves the problem of imbalanced classes, but did not put different focus on hard vs. easy samples.</p>

<h2 id="focal-loss">Focal Loss</h2>
<p>\(FL(p_t) = -\alpha_t(1-p_t)^{\gamma}log(p_t)\)
$p_t$ is the modelâ€™s estimated probability for each class being the correct classification.
$\alpha_T$ is a weighting factor for the class (to further address class imbalance).
$\gamma$ is the focusing parameter that smoothly adjusts the rate at which easy examples are down-weighted. When $\gamma=0$, focal loss is equivalent to cross-entropy loss. As $\gamma=0$ increases, the effect of the modulating factor increases, and the loss contribution from easy examples is reduced.</p>

<p>The key idea behind focal loss is to focus model training on hard, misclassified examples and reduce the relative loss for easy examples, preventing the overwhelming number of easy negatives from dominating the training of the model.</p>

<ul>
  <li>For well-classified examples (where $p_t$ is high), $(1-p_t)^{\gamma}$ approaches 0, which significantly reduces their contribution to the loss. As a result, the loss for these easy examples becomes negligible, and they have little impact on the model updates during training.</li>
  <li>For misclassified or hard examples (where $p_t$ is low), $(1-p_t)^{\gamma}$ is closer to 1, meaning these examples contribute more significantly to the total loss. This ensures that the modelâ€™s updates focus more on correcting these harder examples.</li>
</ul>

<h2 id="poly-loss">Poly Loss</h2>

<h2 id="hinge-loss">Hinge Loss</h2>
<p>Used in SVM. 
\(L(y) = max(0, 1-t \cdot y)\)</p>

<h2 id="triplet-loss">Triplet Loss</h2>

<h2 id="contrastive-loss">Contrastive Loss</h2>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="learning notes" /><category term="machine learning" /><category term="loss function" /><summary type="html"><![CDATA[Regression Loss Functions]]></summary></entry><entry><title type="html">Classification Algorithms</title><link href="http://localhost:4000/posts/2034/08/classification-algorithms/" rel="alternate" type="text/html" title="Classification Algorithms" /><published>2024-03-11T00:00:00+08:00</published><updated>2024-03-11T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/08/classification-algorithms</id><content type="html" xml:base="http://localhost:4000/posts/2034/08/classification-algorithms/"><![CDATA[<p>Classification Algorithms</p>

<h1 id="logistic-regression">Logistic Regression</h1>

<h2 id="description">Description</h2>
<p>\(z = w_0+w_1x_1+w_2x_2+...++w_nx_n\)
Then, the sigmoid function is used to convert z to probability $p$:
\(p = \frac{1}{1+e^{-z}}\)
$p$ can later be converted to binary classification.</p>

<h2 id="pros">Pros</h2>
<ol>
  <li>Easy to implement</li>
  <li>Suitable for linear separable cases</li>
  <li>Result explanable</li>
</ol>

<h2 id="cons">Cons</h2>
<ol>
  <li>Hard to solve non-linear problems</li>
  <li>Performance impaired when dealing highly correlated data</li>
  <li>Sensitive to noisy data</li>
  <li>Not suitable for classification of more than 2 classes</li>
</ol>

<h1 id="decision-tree">Decision Tree</h1>

<h2 id="description-1">Description</h2>
<ol>
  <li>Feature selection</li>
  <li>Tree construction: recursively</li>
</ol>

<h2 id="pros-1">Pros</h2>
<ol>
  <li>Easy and explanation</li>
  <li>Capable of handling both numerical and categorical data</li>
  <li>No need to prepare data, e.g. normalization</li>
</ol>

<h2 id="cons-1">Cons</h2>
<ol>
  <li>Easy to overfit, especially on data with high dimensionality</li>
  <li>Sensitive to outliers</li>
  <li>Unstable, small changes of data could result in large changes of the tree structure</li>
  <li>Not suitable of handling complicated relationship</li>
</ol>

<h2 id="suitable-for">Suitable for</h2>
<ol>
  <li>Cases where explanability is of essence, e.g. medical diagnosis</li>
  <li>Small to medium data size</li>
  <li>Data easy to be splited</li>
  <li>Feature selection</li>
</ol>

<h1 id="random-forest">Random Forest</h1>

<h1 id="naive-bayes">Naive Bayes</h1>

<h2 id="description-2">Description</h2>

<h3 id="assumption">Assumption:</h3>
<p>All features are independent on each other.</p>

<table>
  <tbody>
    <tr>
      <td>$$P(C_k</td>
      <td>x) = \frac{P(x</td>
      <td>C_k)P(C_k)}{P(x)}$$</td>
      <td>Â </td>
      <td>Â </td>
    </tr>
    <tr>
      <td>$$P(x</td>
      <td>C_k) = P(x_1</td>
      <td>C_k)P(x_2</td>
      <td>C_k)â€¦P(x_n</td>
      <td>C_k)$$</td>
    </tr>
  </tbody>
</table>

<h1 id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h1>
<ul>
  <li>Classification: majority vote</li>
  <li>Regression: weighted average</li>
</ul>

<h2 id="distance-measurements">Distance Measurements</h2>
<ul>
  <li>Euclidean Distance</li>
  <li>Manhattan Distance</li>
  <li>Chebyshev Distance</li>
  <li>Minkowski Distance</li>
  <li>Mahalanobis Distance</li>
</ul>

<h2 id="cons-2">Cons:</h2>
<ol>
  <li>Calculation cose</li>
  <li>Space complexity</li>
  <li>Imbalanced classes</li>
  <li>Limited performance on data with high dimensionality</li>
  <li>Sensitive to noisy data</li>
  <li>Hard to decide the correct value of k</li>
</ol>

<h1 id="support-vector-machine">Support Vector Machine</h1>
<p>Linear SVM (Hard Margin)
â€”â€”
\(f(x) = sign(w\cdot x+b)\)</p>

<h2 id="non-linear-kernel-trick">Non-linear (kernel trick)</h2>
<p>\(f(x) = sign(\sum_{i=1}^{n}\alpha_{i}y_iK(x_i, x)+b)\)</p>

<h2 id="svm-for-imbalanced-class-soft-margin">SVM for imbalanced class (Soft Margin)</h2>

<p>this is very easy to accomplish using the aligned environment from amsmath:</p>

\[\begin{equation}
\begin{aligned}
\min_{w,b,\xi} \quad &amp; \frac{1}{2}w^{t}w+C\sum_{i=1}^{N}{\xi_{i}}\\
\textrm{s.t.} \quad &amp; y_{i}(w\phi(x_{i}+b))+\xi_{i}-1\\
  &amp;\xi\geq0    \\
\end{aligned}
\end{equation}\]

<h2 id="pros-2">Pros:</h2>
<ol>
  <li>Can be used in various cases: text classification, image recognition, etc.</li>
  <li>Robust to noisy data points</li>
  <li>Avoid stucking at local optima</li>
  <li>Still works in high dimensional space (kernel trick)</li>
  <li>Avoid over-fitting (by regularization)</li>
</ol>

<h2 id="cons-3">Cons:</h2>
<ol>
  <li>Only work for binary classification case</li>
  <li>High computation complexity</li>
  <li>Sensitive to choice of parameter</li>
  <li>Sensitive to missing data</li>
</ol>

<h1 id="adaboost">AdaBoost</h1>

<h1 id="gradient-boosting-trees">Gradient Boosting Trees</h1>

<h1 id="multilayer-perceptrons">Multilayer Perceptrons</h1>

<p>Artificial Neural Network</p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="learning notes" /><category term="machine learning" /><category term="loss function" /><summary type="html"><![CDATA[Classification Algorithms]]></summary></entry></feed>