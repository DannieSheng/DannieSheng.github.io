<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.9.5">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-06-29T08:31:08+08:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Hudanyun Sheng / Data Scientist</title><subtitle>personal description</subtitle><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><entry><title type="html">Financial Credit Risk Management</title><link href="http://localhost:4000/posts/2024/04/risk-management" rel="alternate" type="text/html" title="Financial Credit Risk Management" /><published>2024-04-02T00:00:00+08:00</published><updated>2024-04-02T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/04/financial-credit-risk-management</id><content type="html" xml:base="http://localhost:4000/posts/2024/04/risk-management"><![CDATA[<h1 id="信贷业务模型维度">信贷业务模型维度</h1>

<ol>
  <li>
    <h2 id="贷前模型">贷前模型</h2>
    <p>A卡 (Application scorecard) 申请评分卡</p>
  </li>
  <li>
    <h2 id="贷中模型">贷中模型</h2>
    <p>B卡 (Behavior scorecard) 行为评分卡</p>
  </li>
  <li>
    <h2 id="贷后模型">贷后模型</h2>
    <p>C卡 (Collection scorecard) 催收评分卡</p>
  </li>
  <li>
    <h2 id="反欺诈模型">反欺诈模型</h2>
  </li>
  <li>
    <h2 id="画像模型">画像模型</h2>
  </li>
</ol>

<h1 id="变量筛选的技术指标">变量筛选的技术指标</h1>

<p>基于缺失率 (Missing Rate)</p>

<p>基于变异系数 (Coefficient of Variation，CV)</p>

<p>基于稳定性 (Population Stability Index，PSI)</p>

<p>基于信息量 (Information Value，IV)</p>

<p>基于RF/XGBoost特征重要性 (Feature Importance)</p>

<p>基于线性相关性 (Linear Correlation)</p>

<p>基于多重共线性 (Multicollinearity)</p>

<p>基于逐步回归 (stepwise)</p>

<p>基于P-Vaule显著性检验</p>

<h1 id="评分卡评估">评分卡评估</h1>

<h2 id="区分度">区分度</h2>
<ul>
  <li>Gini</li>
  <li>AUC</li>
  <li>KS (Kolmogorov-Smirnov)</li>
</ul>

<h2 id="稳定性">稳定性</h2>
<p>PSI (群体稳定性指标)</p>

<h2 id="排序性-ranking">排序性 (Ranking)</h2>
<p>统计bad_rate、lift、Odds等指标，按照评分分数进行分箱。</p>

<h2 id="拟合度-goodness-of-fit">拟合度 (Goodness of Fit)</h2>

<h1 id="其他">其他</h1>

<h2 id="卡方分箱-有监督分箱">卡方分箱 (有监督分箱)</h2>
<p>卡方检验就是对分类数据的频数进行分析的一种方法，它的应用主要表现在两个方面：<strong>拟合优度检验</strong>和<strong>独立性检验（列联分析）</strong>。</p>
<h3 id="拟合优度">拟合优度</h3>

<p>拟合优度是对一个分类变量的检验，即根据总体分布状况，计算出分类变量中各类别的期望频数，与分布的观察频数进行对比，判断期望频数与观察频数是否有显著差异，从而达到对分类变量进行分析的目的。比如，泰坦尼克号中我们观察幸存者是否与性别有关，可以理解为一个X是否与Y有必然联系。</p>

<h3 id="独立性检验">独立性检验</h3>

<p>独立性检验是两个特征变量之间的计算，它可以用来分析两个分类变量是否独立，或者是否有关联。比如某原料质量和产地是否依赖关系，可以理解为一个X与另一个X是否独立。</p>

<h3 id="卡方检验步骤">卡方检验步骤</h3>

<p>(卡方检验的步骤其实就是一般假设检验的过程)</p>

<ol>
  <li>提出假设，比如假设两个变量之间独立</li>
  <li>根据分类的观察频数计算期望频数</li>
  <li>根据卡方公式，计算实际频数与期望频数的卡方值</li>
  <li>根据自由度和事先确定的显著性水平，查找卡方分布表计算卡法值，并与上一步卡方值比较</li>
  <li>得出结果判断是否拒绝原假设</li>
</ol>

<p>卡方分箱是基于<strong>独立性检验</strong>的应用。</p>

<h3 id="卡方分箱步骤">卡方分箱步骤</h3>
<ul>
  <li>初始化</li>
</ul>

<p>根据连续变量值大小进行排序
构建最初的离散化，即把每一个单独的值视为一个箱体。这样做的目的就是想从每个单独的个体开始逐渐合并。</p>
<ul>
  <li>合并
    <ul>
      <li>计算所有相邻分箱的卡方值：也就是说如果有1,2,3,4个分箱，那么就需要绑定相邻的两个分箱，共三组：12,23,34。然后分别计算三个绑定组的卡方值。</li>
      <li>从计算的卡方值中找出最小的一个，并把这两个分箱合并：比如，23是卡方值最小的一个，那么就将2和3合并，本轮计算中分箱就变为了1,23,4。</li>
    </ul>

    <p><strong>基本思想</strong>：如果两个相邻的区间具有非常类似的类分布，那么这两个区间可以合并。否则，它们应该分开。低卡方值表明它们具有相似的类分布。</p>
  </li>
  <li>
    <p>停止条件</p>

    <ul>
      <li>卡方停止的阈值</li>
      <li>分箱数目的限制</li>
    </ul>
  </li>
</ul>

<h2 id="reference">Reference</h2>

<p><a href="https://zhuanlan.zhihu.com/p/370534836">漫谈信贷业务模型体系建设</a> <br />
<a href="https://falbang.com/?p=350">一文看懂风控模型的所有（应该）</a><br />
<a href="https://developer.kingdee.com/article/289708643615246080?productLineId=29&amp;isKnowledge=2&amp;lang=zh-CN&amp;islogin=true,true&amp;global=1">一文带你了解风控评分卡模型</a> (各种指标名称)<br />
<a href="https://cloud.tencent.com/developer/article/1947627?areaId=106001">特征稳定性指标PSI的原理与代码分享</a><br />
<a href="https://cloud.tencent.com/developer/article/1530232">一文介绍特征工程里的卡方分箱，附代码实现</a><br />
<a href="https://blog.csdn.net/richardsz_/article/details/123777141">特征工程 | 信息价值IV与群体稳定性PSI</a>
<a href="https://zhuanlan.zhihu.com/p/80134853">风控模型—WOE与IV指标的深入理解应用</a> (贝叶斯角度)<br />
<a href="https://www.zhihu.com/tardis/zm/art/143472559?source_id=1005">2.2万字，一文看懂风控模型所有</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="financial" /><category term="risk management" /><category term="learning notes" /><summary type="html"><![CDATA[信贷业务模型维度]]></summary></entry><entry><title type="html">Fully Convolutional Networks</title><link href="http://localhost:4000/posts/2024/04/fcn" rel="alternate" type="text/html" title="Fully Convolutional Networks" /><published>2024-04-01T00:00:00+08:00</published><updated>2024-04-01T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/04/FCN</id><content type="html" xml:base="http://localhost:4000/posts/2024/04/fcn"><![CDATA[<p><a href="https://arxiv.org/abs/1411.4038">Fully Convolutional Networks for Semantic Segmentation</a><br />
<a href="https://github.com/shelhamer/fcn.berkeleyvision.org">GitHub</a></p>
<ol>
  <li>
    <p>去除全连接层：
传统的卷积神经网络（CNN）通常包含卷积层和全连接层。在这篇论文中，作者认识到全连接层会导致输出固定大小的特征向量，从而限制了CNN在图像语义分割任务中的应用。
因此，作者去除了CNN中的全连接层，将其替换为全卷积层，使得网络能够接受任意大小的输入图像，并输出相同尺寸的像素级别的分类结果。</p>
  </li>
  <li>
    <p>转换为全卷积网络：
通过将CNN中的全连接层替换为全卷积层，作者将CNN转换为了全卷积网络（FCN）。
全卷积网络具有端到端的特性，能够直接对输入图像进行像素级别的分类，从而实现了图像语义分割任务的端到端映射。</p>
  </li>
  <li>
    <p>使用反卷积（转置卷积）进行上采样：
在FCN中，为了将卷积层的输出恢复到输入图像相同的大小，作者使用了反卷积（deconvolution）进行上采样。
通过反卷积操作，网络能够学习将低分辨率的特征图扩展到与输入图像相同的分辨率，从而得到像素级别的分类结果。</p>
  </li>
  <li>
    <p>添加跳跃连接：
为了提高分割精度，作者在FCN中引入了跳跃连接（skip connections）。
跳跃连接能够将不同层次的语义信息结合起来，从而提高了网络对局部和全局信息的感知能力，进而提高了分割精度。</p>
  </li>
  <li>
    <p>使用多尺度特征融合：
为了进一步提高分割精度，作者还提出了使用多尺度特征融合的方法。
通过将不同层次的特征图进行融合，网络能够同时关注不同尺度的信息，从而提高了对图像语义的理解能力。</p>
  </li>
</ol>

<p><a href="https://mp.weixin.qq.com/s/UbGYZFsJsgBZgtH0-9gL7Q">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="deep learning" /><category term="computer vision" /><category term="learning notes" /><summary type="html"><![CDATA[Fully Convolutional Networks for Semantic Segmentation GitHub 去除全连接层： 传统的卷积神经网络（CNN）通常包含卷积层和全连接层。在这篇论文中，作者认识到全连接层会导致输出固定大小的特征向量，从而限制了CNN在图像语义分割任务中的应用。 因此，作者去除了CNN中的全连接层，将其替换为全卷积层，使得网络能够接受任意大小的输入图像，并输出相同尺寸的像素级别的分类结果。]]></summary></entry><entry><title type="html">Convolutional Neural Networks</title><link href="http://localhost:4000/posts/2024/03/cnn" rel="alternate" type="text/html" title="Convolutional Neural Networks" /><published>2024-03-31T00:00:00+08:00</published><updated>2024-03-31T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/CNN</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/cnn"><![CDATA[<p>什么是卷积神经网络（CNN）？它在计算机视觉中的应用是什么？</p>

<p>卷积神经网络（Convolutional Neural Network，CNN）是一种深度学习模型，特别适用于处理具有网格结构的数据，如图像和视频。
CNN的核心思想是通过卷积操作和池化操作来提取输入数据的特征，并通过这些特征进行分类、识别或回归等任务。</p>

<p>基本原理</p>
<ul>
  <li>
    <p>卷积层（Convolutional Layer）：卷积操作是CNN中的核心部分。它使用一个卷积核（filter）在输入数据上滑动，将卷积核与输入数据的局部区域进行对应元素相乘并求和，生成输出特征图。这一操作能够提取出输入数据的局部特征。</p>
  </li>
  <li>
    <p>激活函数（Activation Function）：通常在卷积层之后会添加一个非线性的激活函数，如ReLU（Rectified Linear Unit），用于引入非线性，增强网络的表达能力。</p>
  </li>
  <li>
    <p>池化层（Pooling Layer）：池化操作用于减小特征图的尺寸并保留其重要信息，常见的池化操作有最大池化（Max Pooling）和平均池化（Average Pooling）等。</p>
  </li>
  <li>
    <p>全连接层（Fully Connected Layer）：在卷积层和池化层之后，通常会连接一个或多个全连接层，用于将特征图展平并连接到输出层，以进行最终的分类或回归。</p>
  </li>
</ul>

<p><a href="https://mp.weixin.qq.com/s/-Qejp91LOImLXZg0VluuMA">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="deep learning" /><category term="computer vision" /><category term="learning notes" /><summary type="html"><![CDATA[什么是卷积神经网络（CNN）？它在计算机视觉中的应用是什么？]]></summary></entry><entry><title type="html">Basic pandas</title><link href="http://localhost:4000/posts/2034/03/basic-pandas/" rel="alternate" type="text/html" title="Basic pandas" /><published>2024-03-22T00:00:00+08:00</published><updated>2024-03-22T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/03/pandas</id><content type="html" xml:base="http://localhost:4000/posts/2034/03/basic-pandas/"><![CDATA[<p>Python, data science, pandas</p>

<p><a href="https://mp.weixin.qq.com/s/0WEoyDAN39KtbhIrgCImeA">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="python" /><category term="data science" /><summary type="html"><![CDATA[Python, data science, pandas]]></summary></entry><entry><title type="html">Basic numpy</title><link href="http://localhost:4000/posts/2034/03/basic-numpy/" rel="alternate" type="text/html" title="Basic numpy" /><published>2024-03-22T00:00:00+08:00</published><updated>2024-03-22T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/03/numpy</id><content type="html" xml:base="http://localhost:4000/posts/2034/03/basic-numpy/"><![CDATA[<p>Python, data science, numpy</p>

<p><a href="https://mp.weixin.qq.com/s/zgcugVeBE2grz9rwqPm8Ig">ref</a></p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="python" /><category term="data science" /><summary type="html"><![CDATA[Python, data science, numpy]]></summary></entry><entry><title type="html">Happy 31th birthday</title><link href="http://localhost:4000/posts/2024/03/31th-birthday/" rel="alternate" type="text/html" title="Happy 31th birthday" /><published>2024-03-15T00:00:00+08:00</published><updated>2024-03-15T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/31th</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/31th-birthday/"><![CDATA[<p>今天不谈机器学习，不再整理一些记不住的知识，而是在三十又一的当下，记录一下最近的思考。</p>

<p>为什么最近会有时间做这么多思考呢？因为我在今年1月，农历新年前不到1个月的时候，试用期还有1周就转正的时候，突然被通知不转正了。</p>

<p>到现在应该失业正好差不多两个月，中间农历新年期间完全是一个招聘停滞的状态。然后在我31岁生日之前至少有一个比较过得去的口头offer，也算不那么焦虑了。</p>

<p>复盘从被解雇到现在，我的心路历程一直非常清晰。最开始小心翼翼降低姿态跟公司争取机会，承认自己有不足之处（这点我现在也不否认，但是，人无完人）；
后来了解到应该去找前公司仲裁，但是因为高昂律师费计划被搁置（但我没有放弃这个想法，毕竟仲裁是一年内都有效的，心里存着一口气）；</p>

<p>再后来跟同期主动离职的同事取得联系（因为不属于一个部门，之前工作没有交集），认识到公司的问题远大于自己的问题，不应该内耗（还有很重要的，他告诉我不要让别人评判自己有没有进步，自己知道自己这几个月是有进步的、是跟之前的自己不一样的，就可以了）；</p>

<p>再接着是回顾在职期间，每天超时工作没有什么别的生活，但是公司还是不满意，当时整个人也严重抑郁，只不过当下被工作的忙碌所填满，没有及时反思，不然可能像前同事一样也应该主动离职了，所以也是好事，但是狗公司这种做法真的过于恶心；
再接下来就是积极找工作，但是这个过程无疑是万分痛苦加上充满了自我怀疑，就因为“试用期<strong>马上转正</strong>被辞退”这一点，就被反复质疑我的工作能力。</p>

<p>对前公司可以说充满了恨，让我在我回国的第一年，加入的第一家公司就接到如此大礼，我真是万分感激🙏<br />
让我在职期间就不断内耗，结果用完就扔<br />
哦对，除了用完就扔，在“通知”我的时候还不忘讽刺恶心我，可是明明就是公司自己不行啊，就是所谓“领导自己能力不行让下属背锅”<br />
让我连续三天没有下床，醒了就哭<br />
让我在农历新年前迷茫，提前回家还要跟我妈假装在远程办公<br />
这件事只跟RC说了，后期还跟一个关系近的前辈（表哥）说了，导致我这段时间也不敢去联系任何朋友（虽然RC说，朋友并不是只有风光的时候才能联系，但我当下的状态就是准备面试与焦虑，可能真的很难跟朋友正常交谈）</p>

<p>但是失业这段时间让我把我好久没有更新的网站给更新了，还开始写博客（主要都是为了自己复习准备面试）。搬家后比较适合户外跑步，也已经连续2周（几乎）每天早起跑步了。也有时间阅读一些“非小说类”书籍（但是说真的，真的每次都有些抗拒去读，因为真的很难懂啊，我说的就是《心理类型》，不过《黑客与画家》还不错）</p>

<p>年纪增长，最近也明显看到了自己身上“岁月的痕迹”：长胖、发腮、眼纹、颈纹、痘印消不了（对啊我可是到30+还在一直长痘呢），消化能力也差……而且最近的状态真是不能说“稳定”：好几次想要跳下去就完了</p>

<p>我会被质疑不会沟通，会被质疑给不了情绪价值，说真的，那我还有什么价值呢？</p>

<p>好吧，写到最后我才承认，这并不是常见的那种充满希望积极向上的30+小姐姐对生活的感悟</p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="personal" /><summary type="html"><![CDATA[今天不谈机器学习，不再整理一些记不住的知识，而是在三十又一的当下，记录一下最近的思考。]]></summary></entry><entry><title type="html">Clustering alrogithms</title><link href="http://localhost:4000/posts/2024/03/stochastic-gradient-descent/" rel="alternate" type="text/html" title="Clustering alrogithms" /><published>2024-03-14T00:00:00+08:00</published><updated>2024-03-14T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/gradient-descent</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/stochastic-gradient-descent/"><![CDATA[<p>gradient descent, stochastic gradient descent, batch gradient descent</p>

<h1 id="improvements-for-gradient-descent">Improvements for gradient descent</h1>
<p>Momentum (惯性保持)
——
\(v_t = \gamma v_{t-1}+ \eta g_t\)
\(\theta_{t+1} = \theta_{t}-v_t\)</p>

<ul>
  <li>学习速率$\eta$乘当前梯度$g_t$.</li>
  <li>带衰减的前一次的步伐$v_{t-1}$.</li>
</ul>

<h2 id="adagrad-环境感知">AdaGrad (环境感知)</h2>
<p>采用“历史梯度平方和”来衡量不同参数的梯度和稀疏性（取值越小越稀疏）</p>

<h2 id="adam-惯性保持环境感知">Adam (惯性保持+环境感知)</h2>
<ul>
  <li>梯度的一阶矩 (first moment)：过往梯度与当前梯度的平均 -&gt; 惯性保持</li>
  <li>梯度的二阶矩 (second moment)：过往梯度平方与当前梯度平方的平均 -&gt; 环境感知，为不同参数产生自适应的学习速率</li>
</ul>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="learning notes" /><summary type="html"><![CDATA[gradient descent, stochastic gradient descent, batch gradient descent]]></summary></entry><entry><title type="html">Activation Functions</title><link href="http://localhost:4000/posts/2024/03/activation-functions/" rel="alternate" type="text/html" title="Activation Functions" /><published>2024-03-13T00:00:00+08:00</published><updated>2024-03-13T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/activation-functions</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/activation-functions/"><![CDATA[<p>激活函数（Activation Function），负责将神经元的输入映射到输出端，激活函数将神经网络中将输入信号的总和转换为输出信号。激活函数大多是非线性函数，才能将多层感知机的输出转换为非线性，使得神经网络可以任意逼近任何非线性函数，进而可以应用到众多的非线性模型中。</p>

<h1 id="sigmoid-family">Sigmoid Family</h1>
<p>\(sigmoid(x) = \frac{1}{1+e^{-x}}\)</p>

<h2 id="hard-sigmoid">Hard Sigmoid</h2>
<p>\(\)</p>

<h2 id="swish">Swish</h2>
<p>\(swish(x) = x\cdot sigmoid(x) = \frac{x}{1+e^{-x}}\)</p>

<h2 id="maxout">Maxout</h2>

<h1 id="relu-family-rectified-linear-unit">ReLU Family (Rectified Linear Unit)</h1>
<p>\(ReLU(x) = max(0,x)\)</p>
<ul>
  <li>Dying neuron</li>
  <li>Handles the vanishing gradient issue</li>
  <li>Cannot avoid exploding gradient issue</li>
</ul>

<h2 id="elu">ELU</h2>
<p>\(ELU(x) = \left\{
\begin{aligned}
x,\ if\ x\ \geq 0 \\
\alpha(e^x-1),\ x&lt;0
\end{aligned}
\right.\)</p>

<ul>
  <li>Avoids dying neuron issue</li>
  <li>Cannot avoid exploding gradient</li>
  <li>Computational expensive (because of exponantial calculation)</li>
  <li>$\alpha$ is an hyper-parameter (normally, $\alpha$ between 0.1 and 0.3)</li>
</ul>

<h2 id="leaky-relu">Leaky ReLU</h2>
<p>\(LeakyReLU(x) = \left\{
\begin{aligned}
x,\ if\ x\ \geq 0 \\
\alpha x,\ x&lt;0
\end{aligned}
\right.\)</p>

<ul>
  <li>Avoids dying neuron issue</li>
  <li>Not computational expensive</li>
  <li>$\alpha$ is an hyper-parameter (normally, $\alpha$ between 0.1 and 0.3)</li>
</ul>

<h2 id="selu-scaled-exponential-linear-units">SELU (Scaled Exponential Linear Units)</h2>
<p>\(SELU(x) = \left\{
\begin{aligned}
\lambda x,\ if\ x\ \geq 0 \\
\lambda \alpha (e^x-1),\ x&lt;0
\end{aligned}
\right.\)
$\alpha \approx 1.6733$, $\lambda \approx 1.0507$.</p>

<h2 id="gelu-gaussian-error-linear-unit">GELU (Gaussian Error Linear Unit)</h2>
<p>\(GELU(x) = xP(X\leq x) = x\Phi(x) = x\cdot\frac{1}{2}[1+erf(\frac{x}{\sqrt(2)})]\)
if $X \sim \mathcal{N}(0,1)$.</p>

<ul>
  <li>GELUs are used in GPT-3, BERT, and most other Transformers.</li>
</ul>

<h1 id="tanh-family">Tanh Family</h1>
<p>\(Tanh(x) = \frac{e^x-e^{-x}}{e^x+e^{-x}}\)</p>
<ul>
  <li>Vanishing gradient problem</li>
  <li>Symmetric about the origin</li>
</ul>

<h2 id="hardtanh">HardTanh</h2>
<p>\(HardTanh(x) = \left\{
\begin{aligned}
-1,\ if\ x &lt; -1\\
x,\ if\ -1\leq x\leq 0\\
1,\ if x&gt;1
\end{aligned}
\right.\)</p>

<ul>
  <li>It is a cheaper and more computationally efficient version of the tanh activation.</li>
</ul>

<h2 id="tanhshrink">TanhShrink</h2>

<h1 id="softmax">Softmax</h1>

<h2 id="logsoftmax">LogSoftMax</h2>

<h2 id="softmin">Softmin</h2>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="machine learning" /><category term="deep learning" /><category term="learning notes" /><summary type="html"><![CDATA[激活函数（Activation Function），负责将神经元的输入映射到输出端，激活函数将神经网络中将输入信号的总和转换为输出信号。激活函数大多是非线性函数，才能将多层感知机的输出转换为非线性，使得神经网络可以任意逼近任何非线性函数，进而可以应用到众多的非线性模型中。]]></summary></entry><entry><title type="html">Loss Functions</title><link href="http://localhost:4000/posts/2024/03/loss-functions/" rel="alternate" type="text/html" title="Loss Functions" /><published>2024-03-12T00:00:00+08:00</published><updated>2024-03-12T00:00:00+08:00</updated><id>http://localhost:4000/posts/2024/03/loss-functions</id><content type="html" xml:base="http://localhost:4000/posts/2024/03/loss-functions/"><![CDATA[<h1 id="regression-loss-functions">Regression Loss Functions</h1>

<h2 id="l1-loss-mean-absolute-error-mse">L1 Loss (Mean Absolute Error, MSE)</h2>
<p>\(MSE = \frac{\sum_{i=1}^{N}|f(x_{i})-y_i|}{N}\)</p>

<h2 id="l2-loss-mean-square-error-mse">L2 Loss (Mean Square Error, MSE)</h2>
<p>\(MSE = \frac{\sum_{i=1}^{N}(f(x_{i})-y_i)^2}{N}\)</p>

<h2 id="smooth-l1-loss">Smooth L1 Loss</h2>
<p>\(SmoothL1 = \left\{
\begin{aligned}
0.5x^2,\ if\ |x|\ &lt;\ 1 \\
|x-0.5|,\ otherwise
\end{aligned}
\right.\)</p>

<h2 id="huber-loss">Huber Loss</h2>
<p>Combines MSE and MAE.</p>

<h1 id="classification-loss-functions">Classification Loss Functions</h1>
<p>Cross Entropy Loss
——</p>

<p>Binary case:
\(CE(p, y) = \left\{
  \begin{aligned}
  -log(p),\ if\ y\ =\ 1, \\
  -log(1-p),\ otherwise
  \end{aligned}
  \right.\)</p>

<p>\(p_t = \left\{
  \begin{aligned}
  p,\ if\ y=1,\\
  1-p,\ otherwise 
  \end{aligned}
\right.\)
Simplified to
\(CE(p,y) = CE(pt) = -log(pt)\)</p>

<p>Multi-class case:
\(CE(p_i, y_i) = -y_{i,c}log(p_{ic})\)</p>

\[L = \frac{1}{N}\sum_{i}L_i = -\frac{1}{N}\sum_{i}\sum_{c=1}^{M}y_{ic}log(p_{ic})\]

<h2 id="balanced-cross-entropy-loss">Balanced Cross Entropy Loss</h2>
<p>\(CE(p_t) = -\alpha_tlog(p_t)\)</p>

<p>Balanced cross entropy loss solves the problem of imbalanced classes, but did not put different focus on hard vs. easy samples.</p>

<h2 id="focal-loss">Focal Loss</h2>
<p>\(FL(p_t) = -\alpha_t(1-p_t)^{\gamma}log(p_t)\)
$p_t$ is the model’s estimated probability for each class being the correct classification.
$\alpha_T$ is a weighting factor for the class (to further address class imbalance).
$\gamma$ is the focusing parameter that smoothly adjusts the rate at which easy examples are down-weighted. When $\gamma=0$, focal loss is equivalent to cross-entropy loss. As $\gamma=0$ increases, the effect of the modulating factor increases, and the loss contribution from easy examples is reduced.</p>

<p>The key idea behind focal loss is to focus model training on hard, misclassified examples and reduce the relative loss for easy examples, preventing the overwhelming number of easy negatives from dominating the training of the model.</p>

<ul>
  <li>For well-classified examples (where $p_t$ is high), $(1-p_t)^{\gamma}$ approaches 0, which significantly reduces their contribution to the loss. As a result, the loss for these easy examples becomes negligible, and they have little impact on the model updates during training.</li>
  <li>For misclassified or hard examples (where $p_t$ is low), $(1-p_t)^{\gamma}$ is closer to 1, meaning these examples contribute more significantly to the total loss. This ensures that the model’s updates focus more on correcting these harder examples.</li>
</ul>

<h2 id="poly-loss">Poly Loss</h2>

<h2 id="hinge-loss">Hinge Loss</h2>
<p>Used in SVM. 
\(L(y) = max(0, 1-t \cdot y)\)</p>

<h2 id="triplet-loss">Triplet Loss</h2>

<h2 id="contrastive-loss">Contrastive Loss</h2>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="learning notes" /><category term="machine learning" /><category term="loss function" /><summary type="html"><![CDATA[Regression Loss Functions]]></summary></entry><entry><title type="html">Classification Algorithms</title><link href="http://localhost:4000/posts/2034/08/classification-algorithms/" rel="alternate" type="text/html" title="Classification Algorithms" /><published>2024-03-11T00:00:00+08:00</published><updated>2024-03-11T00:00:00+08:00</updated><id>http://localhost:4000/posts/2034/08/classification-algorithms</id><content type="html" xml:base="http://localhost:4000/posts/2034/08/classification-algorithms/"><![CDATA[<p>Classification Algorithms</p>

<h1 id="logistic-regression">Logistic Regression</h1>

<h2 id="description">Description</h2>
<p>\(z = w_0+w_1x_1+w_2x_2+...++w_nx_n\)
Then, the sigmoid function is used to convert z to probability $p$:
\(p = \frac{1}{1+e^{-z}}\)
$p$ can later be converted to binary classification.</p>

<h2 id="pros">Pros</h2>
<ol>
  <li>Easy to implement</li>
  <li>Suitable for linear separable cases</li>
  <li>Result explanable</li>
</ol>

<h2 id="cons">Cons</h2>
<ol>
  <li>Hard to solve non-linear problems</li>
  <li>Performance impaired when dealing highly correlated data</li>
  <li>Sensitive to noisy data</li>
  <li>Not suitable for classification of more than 2 classes</li>
</ol>

<h1 id="decision-tree">Decision Tree</h1>

<h2 id="description-1">Description</h2>
<ol>
  <li>Feature selection</li>
  <li>Tree construction: recursively</li>
</ol>

<h2 id="pros-1">Pros</h2>
<ol>
  <li>Easy and explanation</li>
  <li>Capable of handling both numerical and categorical data</li>
  <li>No need to prepare data, e.g. normalization</li>
</ol>

<h2 id="cons-1">Cons</h2>
<ol>
  <li>Easy to overfit, especially on data with high dimensionality</li>
  <li>Sensitive to outliers</li>
  <li>Unstable, small changes of data could result in large changes of the tree structure</li>
  <li>Not suitable of handling complicated relationship</li>
</ol>

<h2 id="suitable-for">Suitable for</h2>
<ol>
  <li>Cases where explanability is of essence, e.g. medical diagnosis</li>
  <li>Small to medium data size</li>
  <li>Data easy to be splited</li>
  <li>Feature selection</li>
</ol>

<h1 id="random-forest">Random Forest</h1>

<h1 id="naive-bayes">Naive Bayes</h1>

<h2 id="description-2">Description</h2>

<h3 id="assumption">Assumption:</h3>
<p>All features are independent on each other.</p>

<table>
  <tbody>
    <tr>
      <td>$$P(C_k</td>
      <td>x) = \frac{P(x</td>
      <td>C_k)P(C_k)}{P(x)}$$</td>
      <td> </td>
      <td> </td>
    </tr>
    <tr>
      <td>$$P(x</td>
      <td>C_k) = P(x_1</td>
      <td>C_k)P(x_2</td>
      <td>C_k)…P(x_n</td>
      <td>C_k)$$</td>
    </tr>
  </tbody>
</table>

<h1 id="k-nearest-neighbors-knn">K-Nearest Neighbors (KNN)</h1>
<ul>
  <li>Classification: majority vote</li>
  <li>Regression: weighted average</li>
</ul>

<h2 id="distance-measurements">Distance Measurements</h2>
<ul>
  <li>Euclidean Distance</li>
  <li>Manhattan Distance</li>
  <li>Chebyshev Distance</li>
  <li>Minkowski Distance</li>
  <li>Mahalanobis Distance</li>
</ul>

<h2 id="cons-2">Cons:</h2>
<ol>
  <li>Calculation cose</li>
  <li>Space complexity</li>
  <li>Imbalanced classes</li>
  <li>Limited performance on data with high dimensionality</li>
  <li>Sensitive to noisy data</li>
  <li>Hard to decide the correct value of k</li>
</ol>

<h1 id="support-vector-machine">Support Vector Machine</h1>
<p>Linear SVM (Hard Margin)
——
\(f(x) = sign(w\cdot x+b)\)</p>

<h2 id="non-linear-kernel-trick">Non-linear (kernel trick)</h2>
<p>\(f(x) = sign(\sum_{i=1}^{n}\alpha_{i}y_iK(x_i, x)+b)\)</p>

<h2 id="svm-for-imbalanced-class-soft-margin">SVM for imbalanced class (Soft Margin)</h2>

<p>this is very easy to accomplish using the aligned environment from amsmath:</p>

\[\begin{equation}
\begin{aligned}
\min_{w,b,\xi} \quad &amp; \frac{1}{2}w^{t}w+C\sum_{i=1}^{N}{\xi_{i}}\\
\textrm{s.t.} \quad &amp; y_{i}(w\phi(x_{i}+b))+\xi_{i}-1\\
  &amp;\xi\geq0    \\
\end{aligned}
\end{equation}\]

<h2 id="pros-2">Pros:</h2>
<ol>
  <li>Can be used in various cases: text classification, image recognition, etc.</li>
  <li>Robust to noisy data points</li>
  <li>Avoid stucking at local optima</li>
  <li>Still works in high dimensional space (kernel trick)</li>
  <li>Avoid over-fitting (by regularization)</li>
</ol>

<h2 id="cons-3">Cons:</h2>
<ol>
  <li>Only work for binary classification case</li>
  <li>High computation complexity</li>
  <li>Sensitive to choice of parameter</li>
  <li>Sensitive to missing data</li>
</ol>

<h1 id="adaboost">AdaBoost</h1>

<h1 id="gradient-boosting-trees">Gradient Boosting Trees</h1>

<h1 id="multilayer-perceptrons">Multilayer Perceptrons</h1>

<p>Artificial Neural Network</p>]]></content><author><name>Hudanyun Sheng</name><email>hudanyun.sheng@outlook.com</email></author><category term="learning notes" /><category term="machine learning" /><category term="loss function" /><summary type="html"><![CDATA[Classification Algorithms]]></summary></entry></feed>