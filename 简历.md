# 联系方式
* 手机：13683132915
* Email: hudanyun.sheng@outlook.com

# 个人信息
* 盛胡丹筠/女/1993
* 硕士/佛罗里达大学电子与计算机工程
* 工作年限：4年
* Github：https://github.com/DannieSheng
* 期望职位：数据科学家
<!-- * 期望薪资：税前月薪25k~40k
* 期望城市：北京 -->

# 教育经历
* 佛罗里达大学 
    * 电子与计算机工程硕士（2018.01 - 2019.12）
    * 硕士论文：基于高光谱图像的柳枝草基因型分类 (Switchgrass Genotype Classification using Hyperspectral Imagery)
    * 工业系统工程硕士（2016.01 - 2017.12）
* 同济大学
    * 物理学学士（2011.09 - 2015.06）
    * 毕业论文： X射线多层膜KB成像的强度均匀性校正

# 工作经历
## Zenni Optical (2023.06 ~ 2024.01)
数据科学家 (其中8月前是兼职)

## 德克萨斯西南医学中心 (2021.09 ~ 2023.05)
数据科学家

## 唐纳德丹福思植物科学中心 (2020.02 ~ 2021.09)
数据科学研究员

## 佛罗里达大学医疗中心 (2019.05 ~ 2019.08)
数据科学实习生

## 项目经历
### 自动处方解析项目（支持日本市场的眼镜处方自动识别）2023.09 ～ 2023.10
- 已有内部开发用于眼镜处方自动解析的工具（OptiReader），该工具的FastAPI服务已经部署在生产环境并且投入使用，可以支持北美市场的处方自动解析。需要需要支持VR眼镜在日本市场的上线。
    - Google Document AI API's Form Parser  （付费，最初解决方案，当下仅仅用于PDF格式的输入）
    - Donut ([OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664))
        - Donut采用了一种创新的OCR-free VDU (Visual Document Understanding)模型，即无需OCR的文档理解模型。Donut采用了Transformer架构，并使用交叉熵损失进行预训练。尽管概念上简单，Donut在实验和分析中表现出色，成为OCR-free VDU领域的领先模型，同时在速度和准确性方面取得了最先进的性能。
        - Donut本身在英文、中文、韩语、日语的数据集上预训练
- 经调研，日本市场主流处方有两种：与北美的眼镜处方除了语言不同外没有差别的横版处方；以及北美市场几乎没有出现的竖版处方
- 调整现有模型，使得现有模型可以正常处理处方中有日语，并且也能处理竖版处方
- 通过调研，竖版处方在日本处方中占较大比重（超过50%），因此为正常解析日本市场的处方，模型调整是必要的；但是日本市场占整个市场的比重比较小，因此花费时间精力重新训练Donut模型（标记数据、训练、调优等等）过于繁琐和不必要
- 为API增加用户输入来表示处方数据来源(`country_of_origin`)，以便于根据来源来区分后续步骤
- 对所有来自日本的处方(`country_of_origin=JP`)，先通过后处理已有Donut模型的输出来判断处方是否是横版
    - 若为横版处方，则正常输出结果
    - 若为竖版处方，则增加一个利用Google Form Parser的步骤，利用现有工具直接解析处方
- 经部署在Dev环境测试后部署上线生产环境投入使用，正常支持日本市场。

### 模糊度检测图像质量测评模块 2023.11 ～ 2024.01
- 从之前生产环境收集到的数据来看，图像本身的质量会极大程度影响处方解析的成功率与准确率 。
    - 已有内部开发的图像质量测评模块，该模块可以检测图像的方向(orientation)。但是仍然需要一个模块来评估图像的模糊程度。
- 设计并训练一个多任务的图像质量评估模块，可以同时评估图像的方向以及模糊程度
- 已有图像方向检测模块是基于ConvNeXt V2 ([OCR-free Document Understanding Transformer](https://arxiv.org/abs/2111.15664))，并且拥有优秀表现。
    - ConvNeXt V2 基于ConvNe模型进行了改进，引入了全卷积的自编码器架构和新的全局响应归一化（GRN）层。是结合了自监督学习技术和架构改进的设计。
    - ConvNeXt V2 在图像分类、目标检测和分割等多个领域都表现出色。特别是在 ImageNet 分类、COCO 目标检测和ADE20K 分割等基准任务上，ConvNeXt V2 在性能上有着显著的提升。
        - 最后，ConvNeXt V2 提供了多个规模的预训练模型，从高效的 3.7M 参数 Atto 模型到 650M 参数的 Huge 模型，都在 ImageNet 上取得了令人瞩目的性能。
    - ConvNeXt V2 的架构改进包括引入全卷积的自编码器框架，这为模型提供了更好的特征学习和表达能力。此外，新的全局响应归一化（GRN）层被添加，以增强通道间特征竞争，从而进一步提高模型的性能。
- 数据标记：
    - 在已有处方解析标签的数据集（用label studio标记）上，通过制定相关标记规则，来产生针对模糊度的标签
    - 但是面临的巨大挑战是，已有数据中，`高度模糊` (`Blurry`) 所占比例相比`Fine` 来说太小，造成类别严重不均衡，不利于分类模型训练。
    - 通过查询生产数据库来收集更多 `Blurry` 数据
        - 包含利用 Streamlit 开发可视化小程序来辅助数据标记  
- 模型设计
    - 针对图像模糊度检测（分类）的模型也选用了ConvNeXt V2作为backbone，选用了hugging face上的[timm/convnextv2_base.fcmae_ft_in22k_in1k_384](https://huggingface.co/timm/convnextv2_base.fcmae_ft_in22k_in1k_384)。
    - 损失函数选了 `Poly1FocalLoss` (提高处理不平衡数据分类的表现)
- 模型表现：
    - `Fine` 准确率：96.2% `Blurry` 准确率：89.6% 
    - `Fine` F1: 95.4% `Blurry` F1 88.9% 

<!-- ### 西门子的“2023年科技促进可持续发展”活动比赛中的“电网中的群体行为”组获胜 (2023.02)
- 随着可再生能源变得更加经济实惠，其实施正导致生产方式越来越分散。此外，能源消费者正演变为拥有各种智能能源资产的生产消费者（prosumers）。能源管理的复杂性显著增加。为了处理这种复杂性，我们需要更好地理解人们的行为，并将其纳入电网管理中。
- 消费者和生产消费者的行为受到很多因素的巨大影响，例如体育赛事、假期或地缘政治事件，这些因素的影响难以预测。这导致管理电网稳定性的不确定性上升。问题在于如何最大化每个人对实现净零排放的贡献，同时最小化对能源网络运营的负担。 -->

### mask R-CNN 用于H&E染色病理图像检测项目 2021.10 ～ 2021.12
- 已有基于keras的mask R-CNN的模型，用于检测非小细胞肺癌(Non-Small Cell Lung Cancer)的染色病理图像中常见的6类细胞核，并同时进行分割和分类
- 通过迁移学习，微调一个用于检测乳腺癌的染色病理图像中常见的7类细胞核，并同时进行分割和分类
- 由于数据的标签存在一定缺陷，不能直接用于训练：
    - Mask R-CNN的需要同时有边界框和（位置）、类别标签、以及像素级别的掩码标签
    - 真实数据的标签有部分缺失———有些数据缺失了掩码标签，有些数据缺失了类别标签（`Unlabeled`）；经统计，这部分标签缺失占总数据的20%（直接舍弃会造成数据浪费）
- 通过重新设计mask R-CNN的损失函数，使得计算损失的时可以有选择地忽略标签有缺失的样本
    - 具体来说，缺少分类标签的样本不参与分类损失的计算，缺少掩码标签的样本不参与掩码损失的计算
-  获得了一个基于PyTorch框架的、适用于检测乳腺癌的染色病理图像中常见的7类细胞核，并同时进行分割和分类的mask R-CNN模型（检测率82.5%，7分类准确率82.0%）
- Mask R-CNN选择原因：
    - Mask R-CNN在实例分割任务中具有高精确度和准确性。在肿瘤细胞核检测中，准确地定位和分割细胞核是至关重要的。
    - Mask R-CNN不仅能够检测物体的边界框，还能生成每个对象实例的详细掩码。对于肿瘤细胞核检测，掩码的生成能力可以提供更详细和精细的信息，有助于更准确地理解细胞核的形状和结构。
    - 基于PyTorch的Mask R-CNN具有灵活性和可调整性，可以根据任务的需求进行修改和调整。这使得它非常适合处理不同形状、尺寸和密度的肿瘤细胞核。
    - Mask R-CNN 是一个受欢迎的模型，有一个庞大而先进的社区支持，拥有丰富的资源、文档和预训练模型。这有助于简化开发过程，加速模型训练和优化过程。

### plantCV 项目 2020.02 ～ 2021.09
- [PlantCV](https://plantcv.danforthcenter.org/)（Plant Computer Vision）是一个专为植物表型分析设计的开源软件包，它基于Python编程语言开发，集成了OpenCV（Open Source Computer Vision Library）等先进的图像处理库。该项目旨在为植物科学家和研究人员提供一个强大、灵活且用户友好的工具，以自动化和量化从各种图像数据中提取的植物表型信息
- 核心功能
    - 图像预处理：PlantCV提供了一系列图像预处理功能，如调整大小、裁剪、背景去除、噪声滤波等，以改善图像质量，为后续分析做准备
    - 特征提取：能够从图像中提取多种植物表型特征，包括但不限于形态学特征（如面积、周长、形状描述符）、颜色特征、纹理特征等
    - 多模态分析：支持处理和分析来自不同成像模式的数据，包括可见光、荧光、红外线和三维成像数据，使其能够适应多种研究需求和实验设计
    - 高通量处理：针对高通量植物表型平台设计，能够自动化处理大量图像数据，提高研究效率和数据处理能力
- 设计理念
    - 开源和社区驱动：PlantCV是一个开源项目，鼓励社区贡献和协作，通过GitHub等平台进行代码共享、问题解答和功能更新
    - 易于使用和扩展：提供了详细的文档、教程和示例代码，帮助用户快速上手并根据自己的研究需求定制和扩展功能
    - 跨学科工具：虽然主要面向植物科学领域，但其强大的图像处理和分析能力也适用于生物学、生态学和农业科学等相关领域的研究
- 应用场景
    - 基因型与表型关联分析：通过量化表型特征，帮助研究人员探索基因型与表型之间的关系
    - 环境应答研究：分析植物对环境变化（如光照、温度、水分等）的表型响应，以研究其适应性和生存策略
    - 品种筛选和育种：量化分析植物表型特征，为植物育种和品种改良提供科学依据

# 技能清单
* 数学和统计学基础：具备扎实的数学原理、概率论和统计学基础，并将这些概念应用于实际情境
* 人工智能和机器学习：机器学习、深度学习和计算机视觉，开发和部署复杂的深度学习和机器学习模型
* 深度学习框架或平台：PyTorch、spaCy、TensorFlow、Keras、Hugging Face
* 编程语言和工具：Python (包括NumPy、Pandas、Scikit-learn、SciPy、ggplot2、Seaborn、OpenCV、Streamlit和plotly等库)、 MATLAB
* DevOps及云技术：Google Cloud、熟练使用Docker进行容器化
* 版本控制：Git
* 数据库和数据存储：MySQL、CloudSQL、BigQuery
* Web开发：Flask框架、FastAPI框架、HTML、CSS、Ajax和JavaScript
* 数据分析：SQL、MS Office、Tableau
* 其他领域：电子健康档案（EHR）数据分析
